#!/usr/bin/env python3
"""
CycleGAN v6.0 - å»éœ§å°ˆç”¨ç‰ˆæœ¬ï¼Œé˜²æ­¢é¢¨æ ¼è½‰æ›åŒ– (å„ªåŒ–ç‰ˆ)
ä¸»è¦æ”¹é€²ï¼š
1. ä¸Šæ¡æ¨£å±¤æ”¹ç‚º Upsample + Conv2d + IN + ReLU
2. ä¿®æ­£ SelfAttention çš„ view å•é¡Œ
3. çœŸæ­£å¥—ç”¨ spectral norm
4. æœ¬åœ° VGG perceptual loss
5. é‡ç–Šæ»‘çª—æ¨è«– + æ¬Šé‡èåˆ
6. å…©å°ºåº¦åˆ¤åˆ¥å™¨
7. çµæ§‹ä¿æŒæå¤± (SSIM + æ¢¯åº¦ä¸€è‡´æ€§ + é‚Šç·£ä¿æŒ) - [0,1]åŸŸè¨ˆç®—
8. å»éœ§å…ˆé©—æå¤± (æœ€å°æ± åŒ–æš—é€šé“ + å°æ¯”åº¦å¢å¼· + äº®åº¦ä¸€è‡´æ€§) - [0,1]åŸŸè¨ˆç®—
9. æå‡ identity loss æ¬Šé‡ (Î»=20) é˜²æ­¢éåº¦é¢¨æ ¼åŒ–
10. å¤šå°ºåº¦ GAN æå¤±å–å‡å€¼
11. åå°„ padding æ›¿ä»£é›¶å¡«å……
12. å­¸ç¿’ç‡ç·šæ€§è¡°æ¸›åˆ°0
13. è¨“ç·´å®Œæˆå¾Œç”Ÿæˆå°ˆæ¥­æå¤±å‡½æ•¸åœ–è¡¨
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.utils.data import DataLoader
from torchvision import transforms
import time
import json
from tqdm import tqdm
import os
import random
from collections import deque
import numpy as np
import matplotlib.pyplot as plt
import matplotlib
matplotlib.use('Agg')
from datetime import datetime
import math

# å°å…¥æ•¸æ“šé›†
from cyclegan_dataset import CycleGANDataset

# è¨­å‚™è¨­å®š
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

def spectral_norm(module, name='weight', power_iterations=1):
    """ä½¿ç”¨ PyTorch å…§å»ºçš„ spectral normalization"""
    try:
        return torch.nn.utils.spectral_norm(module, name=name, n_power_iterations=power_iterations)
    except:
        # å¦‚æœå¤±æ•—ï¼Œè¿”å›åŸå§‹æ¨¡çµ„
        return module

class SelfAttention(nn.Module):
    """ä¿®æ­£çš„è‡ªæ³¨æ„åŠ›æ©Ÿåˆ¶"""
    def __init__(self, in_channels):
        super(SelfAttention, self).__init__()
        self.in_channels = in_channels
        
        # ä½¿ç”¨ spectral norm
        self.query_conv = spectral_norm(nn.Conv2d(in_channels, in_channels // 8, 1))
        self.key_conv = spectral_norm(nn.Conv2d(in_channels, in_channels // 8, 1))
        self.value_conv = spectral_norm(nn.Conv2d(in_channels, in_channels, 1))
        
        self.gamma = nn.Parameter(torch.zeros(1))
        self.softmax = nn.Softmax(dim=-1)
        
    def forward(self, x):
        batch_size, C, H, W = x.size()
        
        # Query, Key, Value
        proj_query = self.query_conv(x).view(batch_size, -1, H * W).permute(0, 2, 1)  # B x N x C'
        proj_key = self.key_conv(x).view(batch_size, -1, H * W)  # B x C' x N
        proj_value = self.value_conv(x).view(batch_size, -1, H * W)  # B x C x N
        
        # æ³¨æ„åŠ›æ¬Šé‡
        attention = torch.bmm(proj_query, proj_key)  # B x N x N
        attention = self.softmax(attention)
        
        # æ‡‰ç”¨æ³¨æ„åŠ›
        out = torch.bmm(proj_value, attention.permute(0, 2, 1))  # B x C x N
        out = out.view(batch_size, C, H, W)  # ä¿®æ­£ï¼šæ­£ç¢ºçš„ view æ“ä½œ
        
        # æ®˜å·®é€£æ¥
        out = self.gamma * out + x
        return out

class ImprovedUpsample(nn.Module):
    """æ”¹é€²çš„ä¸Šæ¡æ¨£å¡Šï¼šUpsample + Conv2d + IN + ReLU"""
    def __init__(self, in_channels, out_channels):
        super(ImprovedUpsample, self).__init__()
        self.upsample = nn.Upsample(scale_factor=2, mode='nearest')
        self.conv = nn.Conv2d(in_channels, out_channels, 3, padding=1)
        self.norm = nn.InstanceNorm2d(out_channels)
        self.relu = nn.ReLU(inplace=True)
        
    def forward(self, x):
        x = self.upsample(x)
        x = self.conv(x)
        x = self.norm(x)
        x = self.relu(x)
        return x

class ResidualBlock(nn.Module):
    """æ®˜å·®å¡Š - ä½¿ç”¨åå°„padding"""
    def __init__(self, channels):
        super(ResidualBlock, self).__init__()
        self.pad1 = nn.ReflectionPad2d(1)
        self.conv1 = nn.Conv2d(channels, channels, 3, padding=0)
        self.norm1 = nn.InstanceNorm2d(channels)
        self.pad2 = nn.ReflectionPad2d(1)
        self.conv2 = nn.Conv2d(channels, channels, 3, padding=0)
        self.norm2 = nn.InstanceNorm2d(channels)
        self.relu = nn.ReLU(inplace=True)
        
    def forward(self, x):
        residual = x
        out = self.pad1(x)
        out = self.conv1(out)
        out = self.norm1(out)
        out = self.relu(out)
        
        out = self.pad2(out)
        out = self.conv2(out)
        out = self.norm2(out)
        return out + residual

class V6Generator(nn.Module):
    """v6 ç”Ÿæˆå™¨ - ä¿®æ­£ä¸Šæ¡æ¨£å’Œæ³¨æ„åŠ›æ©Ÿåˆ¶"""
    def __init__(self, input_channels=3, output_channels=3, n_residual_blocks=9):
        super(V6Generator, self).__init__()
        
        # ç·¨ç¢¼å™¨ - ä½¿ç”¨åå°„padding
        self.encoder = nn.Sequential(
            nn.ReflectionPad2d(3),
            nn.Conv2d(input_channels, 64, 7, padding=0),
            nn.InstanceNorm2d(64),
            nn.ReLU(inplace=True),
            
            nn.Conv2d(64, 128, 3, stride=2, padding=1),
            nn.InstanceNorm2d(128),
            nn.ReLU(inplace=True),
            
            nn.Conv2d(128, 256, 3, stride=2, padding=1),
            nn.InstanceNorm2d(256),
            nn.ReLU(inplace=True),
        )
        
        # æ®˜å·®å¡Š
        residual_layers = []
        for _ in range(n_residual_blocks):
            residual_layers.append(ResidualBlock(256))
        self.residual_blocks = nn.Sequential(*residual_layers)
        
        # è‡ªæ³¨æ„åŠ›ï¼ˆåœ¨ä¸­é–“ç‰¹å¾µåœ–ä¸Šï¼‰
        self.attention = SelfAttention(256)
        
        # è§£ç¢¼å™¨ - å…©æ®µä¸Šæ¡æ¨£ï¼ˆä¿®æ­£å°ºå¯¸å•é¡Œï¼‰+ åå°„padding
        self.decoder = nn.Sequential(
            ImprovedUpsample(256, 128),  # ç¬¬ä¸€æ®µä¸Šæ¡æ¨£ 64x64 -> 128x128
            ImprovedUpsample(128, 64),   # ç¬¬äºŒæ®µä¸Šæ¡æ¨£ 128x128 -> 256x256
            
            nn.ReflectionPad2d(3),
            nn.Conv2d(64, output_channels, 7, padding=0),
            nn.Tanh()
        )
        
    def forward(self, x):
        x = self.encoder(x)
        x = self.residual_blocks(x)
        x = self.attention(x)
        x = self.decoder(x)
        return x

class MultiScaleDiscriminator(nn.Module):
    """å…©å°ºåº¦åˆ¤åˆ¥å™¨"""
    def __init__(self, input_channels=3):
        super(MultiScaleDiscriminator, self).__init__()
        
        # åŸå°ºåº¦åˆ¤åˆ¥å™¨
        self.discriminator_full = self._make_discriminator(input_channels)
        
        # åŠå°ºåº¦åˆ¤åˆ¥å™¨
        self.discriminator_half = self._make_discriminator(input_channels)
        
        self.downsample = nn.AvgPool2d(2)
        
    def _make_discriminator(self, input_channels):
        """å‰µå»ºå–®å€‹åˆ¤åˆ¥å™¨ - åŠ æ·±ä¸€å±¤"""
        return nn.Sequential(
            # ç¬¬ä¸€å±¤
            spectral_norm(nn.Conv2d(input_channels, 64, 4, stride=2, padding=1)),
            nn.LeakyReLU(0.2, inplace=True),
            
            # ç¬¬äºŒå±¤
            spectral_norm(nn.Conv2d(64, 128, 4, stride=2, padding=1)),
            nn.InstanceNorm2d(128),
            nn.LeakyReLU(0.2, inplace=True),
            
            # ç¬¬ä¸‰å±¤
            spectral_norm(nn.Conv2d(128, 256, 4, stride=2, padding=1)),
            nn.InstanceNorm2d(256),
            nn.LeakyReLU(0.2, inplace=True),
            
            # ç¬¬å››å±¤ï¼ˆæ–°å¢ï¼‰
            spectral_norm(nn.Conv2d(256, 512, 4, stride=2, padding=1)),
            nn.InstanceNorm2d(512),
            nn.LeakyReLU(0.2, inplace=True),
            
            # ç¬¬äº”å±¤
            spectral_norm(nn.Conv2d(512, 512, 4, stride=1, padding=1)),
            nn.InstanceNorm2d(512),
            nn.LeakyReLU(0.2, inplace=True),
            
            # è¼¸å‡ºå±¤
            nn.Conv2d(512, 1, 4, stride=1, padding=1)
        )
        
    def forward(self, x):
        # åŸå°ºåº¦
        out_full = self.discriminator_full(x)
        
        # åŠå°ºåº¦
        x_half = self.downsample(x)
        out_half = self.discriminator_half(x_half)
        
        return [out_full, out_half]

class LocalVGGLoss(nn.Module):
    """ç°¡åŒ–çš„æ„ŸçŸ¥æå¤± - æš«æ™‚ç¦ç”¨è¤‡é›œVGG"""
    def __init__(self):
        super(LocalVGGLoss, self).__init__()
        # ç°¡å–®çš„ç‰¹å¾µæå–å™¨
        self.features = nn.Sequential(
            nn.Conv2d(3, 32, 3, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(32, 64, 3, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(2),
            
            nn.Conv2d(64, 64, 3, padding=1),
            nn.ReLU(inplace=True),
        )
        
        # åˆå§‹åŒ–æ¬Šé‡
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')
        
        # å‡çµåƒæ•¸
        for param in self.parameters():
            param.requires_grad = False
            
    def forward(self, input_img, target_img):
        # å¦‚æœè¼¸å…¥åœ–åƒéå°ï¼Œç›´æ¥è¿”å›L1æå¤±
        if input_img.size(2) < 32 or input_img.size(3) < 32:
            return F.l1_loss(input_img, target_img)
        
        try:
            input_features = self.features(input_img)
            target_features = self.features(target_img)
            return F.mse_loss(input_features, target_features)
        except Exception as e:
            # å¦‚æœæœ‰ä»»ä½•éŒ¯èª¤ï¼Œå›é€€åˆ°L1æå¤±
            return F.l1_loss(input_img, target_img)

class StructuralLoss(nn.Module):
    """çµæ§‹ä¿æŒæå¤± - é˜²æ­¢éåº¦é¢¨æ ¼åŒ–"""
    def __init__(self):
        super(StructuralLoss, self).__init__()
        
    def forward(self, input_img, output_img):
        # è½‰æ›åˆ° [0,1] åŸŸé€²è¡Œè¨ˆç®—
        input_01 = (input_img + 1.0) / 2.0  # [-1,1] -> [0,1]
        output_01 = (output_img + 1.0) / 2.0  # [-1,1] -> [0,1]
        
        # SSIM çµæ§‹ç›¸ä¼¼æ€§æå¤±
        ssim_loss = self._ssim_loss(input_01, output_01)
        
        # æ¢¯åº¦ä¸€è‡´æ€§æå¤±
        gradient_loss = self._gradient_loss(input_01, output_01)
        
        # é‚Šç·£ä¿æŒæå¤±
        edge_loss = self._edge_preserving_loss(input_01, output_01)
        
        return ssim_loss + 0.5 * gradient_loss + 0.3 * edge_loss
    
    def _ssim_loss(self, img1, img2):
        """SSIM æå¤±"""
        mu1 = F.avg_pool2d(img1, kernel_size=3, stride=1, padding=1)
        mu2 = F.avg_pool2d(img2, kernel_size=3, stride=1, padding=1)
        
        mu1_sq = mu1.pow(2)
        mu2_sq = mu2.pow(2)
        mu1_mu2 = mu1 * mu2
        
        sigma1_sq = F.avg_pool2d(img1 * img1, kernel_size=3, stride=1, padding=1) - mu1_sq
        sigma2_sq = F.avg_pool2d(img2 * img2, kernel_size=3, stride=1, padding=1) - mu2_sq
        sigma12 = F.avg_pool2d(img1 * img2, kernel_size=3, stride=1, padding=1) - mu1_mu2
        
        c1 = 0.01 ** 2
        c2 = 0.03 ** 2
        
        ssim_map = ((2 * mu1_mu2 + c1) * (2 * sigma12 + c2)) / ((mu1_sq + mu2_sq + c1) * (sigma1_sq + sigma2_sq + c2))
        
        return 1 - ssim_map.mean()
    
    def _gradient_loss(self, img1, img2):
        """æ¢¯åº¦ä¸€è‡´æ€§æå¤±"""
        def gradient(img):
            # Sobel æ¢¯åº¦
            sobel_x = torch.tensor([[-1, 0, 1], [-2, 0, 2], [-1, 0, 1]], dtype=torch.float32).view(1, 1, 3, 3).to(img.device)
            sobel_y = torch.tensor([[-1, -2, -1], [0, 0, 0], [1, 2, 1]], dtype=torch.float32).view(1, 1, 3, 3).to(img.device)
            
            # å°‡åœ–åƒè½‰æ›ç‚ºç°åº¦
            gray = 0.299 * img[:, 0:1] + 0.587 * img[:, 1:2] + 0.114 * img[:, 2:3]
            
            grad_x = F.conv2d(gray, sobel_x, padding=1)
            grad_y = F.conv2d(gray, sobel_y, padding=1)
            
            return torch.sqrt(grad_x**2 + grad_y**2 + 1e-8)
        
        grad1 = gradient(img1)
        grad2 = gradient(img2)
        
        return F.l1_loss(grad1, grad2)
    
    def _edge_preserving_loss(self, img1, img2):
        """é‚Šç·£ä¿æŒæå¤±"""
        # Laplacian é‚Šç·£æª¢æ¸¬
        laplacian = torch.tensor([[0, -1, 0], [-1, 4, -1], [0, -1, 0]], dtype=torch.float32).view(1, 1, 3, 3).to(img1.device)
        
        # è½‰æ›ç‚ºç°åº¦
        gray1 = 0.299 * img1[:, 0:1] + 0.587 * img1[:, 1:2] + 0.114 * img1[:, 2:3]
        gray2 = 0.299 * img2[:, 0:1] + 0.587 * img2[:, 1:2] + 0.114 * img2[:, 2:3]
        
        edge1 = F.conv2d(gray1, laplacian, padding=1)
        edge2 = F.conv2d(gray2, laplacian, padding=1)
        
        return F.l1_loss(edge1, edge2)

class DehazingPriorLoss(nn.Module):
    """å»éœ§å…ˆé©—æå¤± - åŸºæ–¼å¤§æ°£æ•£å°„æ¨¡å‹å’Œæš—é€šé“å…ˆé©—"""
    def __init__(self):
        super(DehazingPriorLoss, self).__init__()
        
    def forward(self, hazy_img, dehazed_img):
        # è½‰æ›åˆ° [0,1] åŸŸé€²è¡Œè¨ˆç®—
        hazy_01 = (hazy_img + 1.0) / 2.0  # [-1,1] -> [0,1]
        dehazed_01 = (dehazed_img + 1.0) / 2.0  # [-1,1] -> [0,1]
        
        # æš—é€šé“å…ˆé©—æå¤±
        dark_channel_loss = self._dark_channel_loss(hazy_01, dehazed_01)
        
        # å°æ¯”åº¦å¢å¼·æå¤±
        contrast_loss = self._contrast_enhancement_loss(hazy_01, dehazed_01)
        
        # äº®åº¦ä¸€è‡´æ€§æå¤±
        brightness_loss = self._brightness_consistency_loss(hazy_01, dehazed_01)
        
        # é¡è‰²åç§»æå¤±
        color_shift_loss = self._color_shift_loss(hazy_01, dehazed_01)
        
        return dark_channel_loss + 0.5 * contrast_loss + 0.3 * brightness_loss + 0.2 * color_shift_loss
    
    def _dark_channel_loss(self, hazy, dehazed):
        """æš—é€šé“å…ˆé©—ï¼šå»éœ§å¾Œçš„æš—é€šé“æ‡‰è©²æ›´å°ï¼Œä½¿ç”¨æœ€å°æ± åŒ–"""
        def dark_channel(img):
            # æ¯å€‹åƒç´ åœ¨RGBä¸‰å€‹é€šé“ä¸­çš„æœ€å°å€¼
            dc = torch.min(img, dim=1, keepdim=True)[0]
            # ä½¿ç”¨æœ€å°æ± åŒ–ï¼ˆé€šéè² çš„æœ€å¤§æ± åŒ–å¯¦ç¾ï¼‰
            dc_neg = -dc
            dc_pooled = F.max_pool2d(dc_neg, kernel_size=15, stride=1, padding=7)
            dc = -dc_pooled  # è½‰å›æœ€å°æ± åŒ–çµæœ
            return dc
        
        dc_hazy = dark_channel(hazy)
        dc_dehazed = dark_channel(dehazed)
        
        # å»éœ§å¾Œçš„æš—é€šé“æ‡‰è©²æ›´å°ï¼ˆæ›´æ¥è¿‘0ï¼‰
        loss = F.relu(dc_dehazed - dc_hazy + 0.1).mean()
        
        return loss
    
    def _contrast_enhancement_loss(self, hazy, dehazed):
        """å°æ¯”åº¦å¢å¼·ï¼šå»éœ§å¾Œå°æ¯”åº¦æ‡‰è©²æ›´é«˜"""
        def local_contrast(img):
            # è¨ˆç®—å±€éƒ¨æ¨™æº–å·®ä½œç‚ºå°æ¯”åº¦æŒ‡æ¨™
            mean = F.avg_pool2d(img, kernel_size=9, stride=1, padding=4)
            sq_diff = (img - mean) ** 2
            variance = F.avg_pool2d(sq_diff, kernel_size=9, stride=1, padding=4)
            std = torch.sqrt(variance + 1e-8)
            return std.mean()
        
        contrast_hazy = local_contrast(hazy)
        contrast_dehazed = local_contrast(dehazed)
        
        # å»éœ§å¾Œå°æ¯”åº¦æ‡‰è©²æ›´é«˜
        loss = F.relu(contrast_hazy - contrast_dehazed + 0.05)
        
        return loss
    
    def _brightness_consistency_loss(self, hazy, dehazed):
        """äº®åº¦ä¸€è‡´æ€§ï¼šé˜²æ­¢éåº¦æ›å…‰æˆ–éæš—"""
        # è¨ˆç®—äº®åº¦ï¼ˆè½‰ç‚ºç°åº¦ï¼‰
        def to_grayscale(img):
            return 0.299 * img[:, 0:1] + 0.587 * img[:, 1:2] + 0.114 * img[:, 2:3]
        
        brightness_hazy = to_grayscale(hazy).mean()
        brightness_dehazed = to_grayscale(dehazed).mean()
        
        # äº®åº¦è®ŠåŒ–æ‡‰è©²é©ä¸­ï¼ˆä¸è¦éåº¦è®ŠåŒ–ï¼‰
        brightness_diff = torch.abs(brightness_dehazed - brightness_hazy)
        
        # æ‡²ç½°éåº¦çš„äº®åº¦è®ŠåŒ–ï¼ˆ>0.3ï¼‰
        loss = F.relu(brightness_diff - 0.3)
        
        return loss
    
    def _color_shift_loss(self, hazy, dehazed):
        """é¡è‰²åç§»æå¤±ï¼šé˜²æ­¢é¡è‰²éåº¦åç§»"""
        # è¨ˆç®—RGBå„é€šé“çš„å‡å€¼
        hazy_mean = hazy.mean(dim=[2, 3], keepdim=True)  # [B, 3, 1, 1]
        dehazed_mean = dehazed.mean(dim=[2, 3], keepdim=True)
        
        # é¡è‰²åç§»æ‡‰è©²é©ä¸­
        color_diff = torch.abs(dehazed_mean - hazy_mean)
        
        # æ‡²ç½°éåº¦çš„é¡è‰²åç§»
        loss = F.relu(color_diff - 0.2).mean()
        
        return loss

class ImageBuffer:
    """æ­·å²åœ–ç‰‡ç·©è¡å€"""
    def __init__(self, buffer_size=50):
        self.buffer_size = buffer_size
        self.buffer = []
    
    def push_and_pop(self, images):
        to_return = []
        for image in images:
            image = torch.unsqueeze(image.data, 0)
            if len(self.buffer) < self.buffer_size:
                self.buffer.append(image)
                to_return.append(image)
            else:
                if random.uniform(0, 1) > 0.5:
                    i = random.randint(0, self.buffer_size - 1)
                    to_return.append(self.buffer[i].clone())
                    self.buffer[i] = image
                else:
                    to_return.append(image)
        return torch.cat(to_return)

def init_weights(net, init_type='normal', init_gain=0.02):
    """åˆå§‹åŒ–ç¶²è·¯æ¬Šé‡"""
    def init_func(m):
        classname = m.__class__.__name__
        if hasattr(m, 'weight') and m.weight is not None and (classname.find('Conv') != -1 or classname.find('Linear') != -1):
            if init_type == 'normal':
                torch.nn.init.normal_(m.weight.data, 0.0, init_gain)
            elif init_type == 'xavier':
                torch.nn.init.xavier_normal_(m.weight.data, gain=init_gain)
            elif init_type == 'kaiming':
                torch.nn.init.kaiming_normal_(m.weight.data, a=0, mode='fan_in')
            if hasattr(m, 'bias') and m.bias is not None:
                torch.nn.init.constant_(m.bias.data, 0.0)
        elif (classname.find('BatchNorm2d') != -1 or classname.find('InstanceNorm2d') != -1):
            if hasattr(m, 'weight') and m.weight is not None:
                torch.nn.init.normal_(m.weight.data, 1.0, init_gain)
            if hasattr(m, 'bias') and m.bias is not None:
                torch.nn.init.constant_(m.bias.data, 0.0)
    
    net.apply(init_func)

class OverlapInference:
    """é‡ç–Šæ»‘çª—æ¨è«– + æ¬Šé‡èåˆ"""
    def __init__(self, model, tile_size=512, overlap=64):
        self.model = model
        self.tile_size = tile_size
        self.overlap = overlap
        self.stride = tile_size - overlap
        
    def __call__(self, input_tensor):
        """é‡ç–Šæ»‘çª—æ¨è«–"""
        batch_size, channels, height, width = input_tensor.shape
        
        # å¦‚æœåœ–åƒå°æ–¼tile_sizeï¼Œç›´æ¥è™•ç†
        if height <= self.tile_size and width <= self.tile_size:
            return self.model(input_tensor)
        
        # è¨ˆç®—éœ€è¦çš„tiles
        h_tiles = math.ceil((height - self.overlap) / self.stride)
        w_tiles = math.ceil((width - self.overlap) / self.stride)
        
        # è¼¸å‡ºtensorå’Œæ¬Šé‡tensor
        output = torch.zeros_like(input_tensor)
        weights = torch.zeros(batch_size, 1, height, width, device=input_tensor.device)
        
        for h in range(h_tiles):
            for w in range(w_tiles):
                # è¨ˆç®—tileä½ç½®
                h_start = h * self.stride
                w_start = w * self.stride
                h_end = min(h_start + self.tile_size, height)
                w_end = min(w_start + self.tile_size, width)
                
                # æå–tile
                tile = input_tensor[:, :, h_start:h_end, w_start:w_end]
                
                # å¦‚æœtileå¤ªå°ï¼Œè·³é
                if tile.shape[2] < 64 or tile.shape[3] < 64:
                    continue
                
                # è™•ç†tile
                with torch.no_grad():
                    tile_output = self.model(tile)
                
                # è¨ˆç®—å°ç¨±æ¬Šé‡ï¼ˆå››é‚Šæ·¡å‡ºï¼‰
                tile_h, tile_w = tile.shape[2], tile.shape[3]
                tile_weight = torch.ones(1, 1, tile_h, tile_w, device=input_tensor.device)
                
                # å°ç¨±é‚Šç·£æ·¡å‡º
                fade_size = self.overlap // 2
                if fade_size > 0:
                    # ä¸Šä¸‹é‚Šç·£å°ç¨±æ·¡å‡º
                    if h > 0 or h < h_tiles - 1:
                        fade = torch.linspace(0, 1, fade_size, device=input_tensor.device)
                        # ä¸Šé‚Šç·£
                        if h > 0:
                            tile_weight[:, :, :fade_size, :] *= fade.view(-1, 1)
                        # ä¸‹é‚Šç·£
                        if h < h_tiles - 1:
                            tile_weight[:, :, -fade_size:, :] *= fade.flip(0).view(-1, 1)
                    
                    # å·¦å³é‚Šç·£å°ç¨±æ·¡å‡º
                    if w > 0 or w < w_tiles - 1:
                        fade = torch.linspace(0, 1, fade_size, device=input_tensor.device)
                        # å·¦é‚Šç·£
                        if w > 0:
                            tile_weight[:, :, :, :fade_size] *= fade.view(1, -1)
                        # å³é‚Šç·£
                        if w < w_tiles - 1:
                            tile_weight[:, :, :, -fade_size:] *= fade.flip(0).view(1, -1)
                
                # ç´¯ç©çµæœ
                output[:, :, h_start:h_end, w_start:w_end] += tile_output * tile_weight
                weights[:, :, h_start:h_end, w_start:w_end] += tile_weight
        
        # æ­£è¦åŒ–
        weights = torch.clamp(weights, min=1e-8)
        output = output / weights
        
        return output

class V6CycleGANTrainer:
    """v6 CycleGAN è¨“ç·´å™¨ - å°ˆç‚ºå»éœ§ä»»å‹™å„ªåŒ–"""
    def __init__(self, 
                 lambda_cycle=10.0, 
                 lambda_identity=20.0,  # æå‡identityæ¬Šé‡é˜²æ­¢é¢¨æ ¼åŒ–
                 lambda_perceptual=1.0,
                 lambda_structural=5.0,  # çµæ§‹ä¿æŒæå¤±æ¬Šé‡
                 lambda_dehazing=3.0,   # å»éœ§å…ˆé©—æå¤±æ¬Šé‡
                 buffer_size=50, 
                 lr=0.0002, 
                 beta1=0.5, 
                 beta2=0.999):
        
        # å‰µå»ºæ¨¡å‹
        self.G_A2B = V6Generator().to(device)
        self.G_B2A = V6Generator().to(device)
        self.D_A = MultiScaleDiscriminator().to(device)
        self.D_B = MultiScaleDiscriminator().to(device)
        
        # åˆå§‹åŒ–æ¬Šé‡
        init_weights(self.G_A2B)
        init_weights(self.G_B2A)
        init_weights(self.D_A)
        init_weights(self.D_B)
        
        # æå¤±å‡½æ•¸
        self.criterion_GAN = nn.MSELoss()
        self.criterion_cycle = nn.L1Loss()
        self.criterion_identity = nn.L1Loss()
        self.criterion_perceptual = LocalVGGLoss().to(device)
        self.criterion_structural = StructuralLoss().to(device)  # çµæ§‹ä¿æŒæå¤±
        self.criterion_dehazing = DehazingPriorLoss().to(device)  # å»éœ§å…ˆé©—æå¤±
        
        # å„ªåŒ–å™¨
        self.optimizer_G = optim.Adam(
            list(self.G_A2B.parameters()) + list(self.G_B2A.parameters()),
            lr=lr, betas=(beta1, beta2))
        self.optimizer_D_A = optim.Adam(self.D_A.parameters(), lr=lr, betas=(beta1, beta2))
        self.optimizer_D_B = optim.Adam(self.D_B.parameters(), lr=lr, betas=(beta1, beta2))
        
        # å­¸ç¿’ç‡èª¿åº¦å™¨ - LRç·šæ€§è¡°æ¸›åˆ°0
        self.scheduler_G = optim.lr_scheduler.LambdaLR(
            self.optimizer_G, lr_lambda=lambda epoch: max(0.0, 1.0 - max(0, epoch - 100) / 100))
        self.scheduler_D_A = optim.lr_scheduler.LambdaLR(
            self.optimizer_D_A, lr_lambda=lambda epoch: max(0.0, 1.0 - max(0, epoch - 100) / 100))
        self.scheduler_D_B = optim.lr_scheduler.LambdaLR(
            self.optimizer_D_B, lr_lambda=lambda epoch: max(0.0, 1.0 - max(0, epoch - 100) / 100))
        
        # ç·©è¡å€
        self.fake_A_buffer = ImageBuffer(buffer_size)
        self.fake_B_buffer = ImageBuffer(buffer_size)
        
        # åƒæ•¸
        self.lambda_cycle = lambda_cycle
        self.lambda_identity = lambda_identity
        self.lambda_perceptual = lambda_perceptual
        self.lambda_structural = lambda_structural
        self.lambda_dehazing = lambda_dehazing
        
        # é‡ç–Šæ¨è«–å™¨
        self.overlap_inference_A2B = OverlapInference(self.G_A2B)
        self.overlap_inference_B2A = OverlapInference(self.G_B2A)
        
        # æå¤±è¨˜éŒ„
        self.losses_history = {
            'G_loss': [],
            'D_A_loss': [],
            'D_B_loss': [],
            'cycle_loss': [],
            'identity_loss': [],
            'perceptual_loss': [],
            'structural_loss': [],
            'dehazing_loss': []
        }
    
    def train_epoch(self, dataloader, epoch):
        """è¨“ç·´ä¸€å€‹ epoch"""
        self.G_A2B.train()
        self.G_B2A.train()
        self.D_A.train()
        self.D_B.train()
        
        epoch_losses = {
            'G_loss': 0, 'D_A_loss': 0, 'D_B_loss': 0,
            'cycle_loss': 0, 'identity_loss': 0, 'perceptual_loss': 0,
            'structural_loss': 0, 'dehazing_loss': 0
        }
        
        for i, (real_A, real_B) in enumerate(tqdm(dataloader, desc=f"Epoch {epoch}")):
            real_A, real_B = real_A.to(device), real_B.to(device)
            
            # ç”Ÿæˆå‡åœ–åƒ
            fake_B = self.G_A2B(real_A)
            fake_A = self.G_B2A(real_B)
            
            # è¨“ç·´ç”Ÿæˆå™¨
            self.optimizer_G.zero_grad()
            
            # GAN æå¤± - å¤šå°ºåº¦æå¤±å–å‡å€¼
            pred_fake_A = self.D_A(fake_A)
            pred_fake_B = self.D_B(fake_B)
            
            loss_GAN_A2B = sum([self.criterion_GAN(pred, torch.ones_like(pred)) for pred in pred_fake_B]) / len(pred_fake_B)
            loss_GAN_B2A = sum([self.criterion_GAN(pred, torch.ones_like(pred)) for pred in pred_fake_A]) / len(pred_fake_A)
            
            # å¾ªç’°ä¸€è‡´æ€§æå¤±
            recovered_A = self.G_B2A(fake_B)
            recovered_B = self.G_A2B(fake_A)
            loss_cycle_A = self.criterion_cycle(recovered_A, real_A)
            loss_cycle_B = self.criterion_cycle(recovered_B, real_B)
            loss_cycle = (loss_cycle_A + loss_cycle_B) * self.lambda_cycle
            
            # èº«ä»½æå¤±
            identity_A = self.G_B2A(real_A)
            identity_B = self.G_A2B(real_B)
            loss_identity_A = self.criterion_identity(identity_A, real_A)
            loss_identity_B = self.criterion_identity(identity_B, real_B)
            loss_identity = (loss_identity_A + loss_identity_B) * self.lambda_identity
            
            # æ„ŸçŸ¥æå¤±
            loss_perceptual_A = self.criterion_perceptual(fake_B, real_B)
            loss_perceptual_B = self.criterion_perceptual(fake_A, real_A)
            loss_perceptual = (loss_perceptual_A + loss_perceptual_B) * self.lambda_perceptual
            
            # çµæ§‹ä¿æŒæå¤±ï¼ˆé˜²æ­¢éåº¦é¢¨æ ¼åŒ–ï¼‰
            loss_structural_A = self.criterion_structural(real_A, fake_B)  # A->B çµæ§‹ä¿æŒ
            loss_structural_B = self.criterion_structural(real_B, fake_A)  # B->A çµæ§‹ä¿æŒ
            loss_structural = (loss_structural_A + loss_structural_B) * self.lambda_structural
            
            # å»éœ§å…ˆé©—æå¤±ï¼ˆåƒ…é©ç”¨æ–¼ A->Bï¼Œå³æœ‰éœ§->æ¸…æ™°ï¼‰
            loss_dehazing = self.criterion_dehazing(real_A, fake_B) * self.lambda_dehazing
            
            # ç¸½ç”Ÿæˆå™¨æå¤±
            loss_G = loss_GAN_A2B + loss_GAN_B2A + loss_cycle + loss_identity + loss_perceptual + loss_structural + loss_dehazing
            loss_G.backward()
            self.optimizer_G.step()
            
            # è¨“ç·´åˆ¤åˆ¥å™¨ A
            self.optimizer_D_A.zero_grad()
            
            pred_real_A = self.D_A(real_A)
            loss_D_A_real = sum([self.criterion_GAN(pred, torch.ones_like(pred)) for pred in pred_real_A]) / len(pred_real_A)
            
            fake_A_buffered = self.fake_A_buffer.push_and_pop(fake_A)
            pred_fake_A = self.D_A(fake_A_buffered.detach())
            loss_D_A_fake = sum([self.criterion_GAN(pred, torch.zeros_like(pred)) for pred in pred_fake_A]) / len(pred_fake_A)
            
            loss_D_A = (loss_D_A_real + loss_D_A_fake) * 0.5
            loss_D_A.backward()
            self.optimizer_D_A.step()
            
            # è¨“ç·´åˆ¤åˆ¥å™¨ B
            self.optimizer_D_B.zero_grad()
            
            pred_real_B = self.D_B(real_B)
            loss_D_B_real = sum([self.criterion_GAN(pred, torch.ones_like(pred)) for pred in pred_real_B]) / len(pred_real_B)
            
            fake_B_buffered = self.fake_B_buffer.push_and_pop(fake_B)
            pred_fake_B = self.D_B(fake_B_buffered.detach())
            loss_D_B_fake = sum([self.criterion_GAN(pred, torch.zeros_like(pred)) for pred in pred_fake_B]) / len(pred_fake_B)
            
            loss_D_B = (loss_D_B_real + loss_D_B_fake) * 0.5
            loss_D_B.backward()
            self.optimizer_D_B.step()
            
            # ç´¯ç©æå¤±
            epoch_losses['G_loss'] += loss_G.item()
            epoch_losses['D_A_loss'] += loss_D_A.item()
            epoch_losses['D_B_loss'] += loss_D_B.item()
            epoch_losses['cycle_loss'] += loss_cycle.item()
            epoch_losses['identity_loss'] += loss_identity.item()
            epoch_losses['perceptual_loss'] += loss_perceptual.item()
            epoch_losses['structural_loss'] += loss_structural.item()
            epoch_losses['dehazing_loss'] += loss_dehazing.item()
            
            if (i + 1) % 100 == 0:
                print(f"Batch [{i+1}] - G: {loss_G.item():.4f}, D_A: {loss_D_A.item():.4f}, "
                      f"D_B: {loss_D_B.item():.4f}, Cycle: {loss_cycle.item():.4f}, "
                      f"Struct: {loss_structural.item():.4f}, DCP: {loss_dehazing.item():.4f}")
        
        # è¨ˆç®—å¹³å‡æå¤±
        for key in epoch_losses:
            epoch_losses[key] /= len(dataloader)
        
        return epoch_losses
    
    def save_checkpoint(self, epoch):
        """ä¿å­˜æª¢æŸ¥é»"""
        os.makedirs('checkpoints', exist_ok=True)
        checkpoint = {
            'epoch': epoch,
            'G_A2B_state_dict': self.G_A2B.state_dict(),
            'G_B2A_state_dict': self.G_B2A.state_dict(),
            'D_A_state_dict': self.D_A.state_dict(),
            'D_B_state_dict': self.D_B.state_dict(),
            'optimizer_G_state_dict': self.optimizer_G.state_dict(),
            'optimizer_D_A_state_dict': self.optimizer_D_A.state_dict(),
            'optimizer_D_B_state_dict': self.optimizer_D_B.state_dict(),
            'losses_history': self.losses_history
        }
        torch.save(checkpoint, f'checkpoints/cyclegan_v6_epoch_{epoch}.pth')
        print(f"âœ… æª¢æŸ¥é»å·²ä¿å­˜: cyclegan_v6_epoch_{epoch}.pth")
    
    def plot_losses(self):
        """ç”Ÿæˆå„ªåŒ–ç‰ˆæå¤±å‡½æ•¸åœ–è¡¨ - åŒ…å« DCP æå¤±"""
        plt.figure(figsize=(20, 12))
        
        # æ·»åŠ ç¸½æ¨™é¡Œ
        plt.suptitle('CycleGAN v6.0 å„ªåŒ–ç‰ˆæå¤±å‡½æ•¸åˆ†æ ([0,1]åŸŸ + æœ€å°æ± åŒ– + åå°„Padding + LRâ†’0)', 
                    fontsize=16, fontweight='bold')
        
        # å‰µå»ºå­åœ– (3x3 layout)
        plt.subplot(3, 3, 1)
        plt.plot(self.losses_history['G_loss'], label='Generator Loss', color='blue')
        plt.title('Generator Loss')
        plt.xlabel('Epoch')
        plt.ylabel('Loss')
        plt.legend()
        plt.grid(True)
        
        plt.subplot(3, 3, 2)
        plt.plot(self.losses_history['D_A_loss'], label='Discriminator A Loss', color='red')
        plt.plot(self.losses_history['D_B_loss'], label='Discriminator B Loss', color='orange')
        plt.title('Discriminator Losses')
        plt.xlabel('Epoch')
        plt.ylabel('Loss')
        plt.legend()
        plt.grid(True)
        
        plt.subplot(3, 3, 3)
        plt.plot(self.losses_history['cycle_loss'], label='Cycle Loss', color='green')
        plt.title('Cycle Consistency Loss')
        plt.xlabel('Epoch')
        plt.ylabel('Loss')
        plt.legend()
        plt.grid(True)
        
        plt.subplot(3, 3, 4)
        plt.plot(self.losses_history['identity_loss'], label='Identity Loss (High Weight)', color='purple')
        plt.title('Identity Loss (Î»=20)')
        plt.xlabel('Epoch')
        plt.ylabel('Loss')
        plt.legend()
        plt.grid(True)
        
        plt.subplot(3, 3, 5)
        plt.plot(self.losses_history['perceptual_loss'], label='Perceptual Loss', color='brown')
        plt.title('Perceptual Loss')
        plt.xlabel('Epoch')
        plt.ylabel('Loss')
        plt.legend()
        plt.grid(True)
        
        plt.subplot(3, 3, 6)
        plt.plot(self.losses_history['structural_loss'], label='Structural Loss', color='cyan')
        plt.title('Structural Preservation Loss')
        plt.xlabel('Epoch')
        plt.ylabel('Loss')
        plt.legend()
        plt.grid(True)
        
        plt.subplot(3, 3, 7)
        plt.plot(self.losses_history['dehazing_loss'], label='DCP Loss (Dehazing Prior)', color='magenta')
        plt.title('DCP Loss - æš—é€šé“å…ˆé©—å»éœ§æå¤±')
        plt.xlabel('Epoch')
        plt.ylabel('Loss')
        plt.legend()
        plt.grid(True)
        
        plt.subplot(3, 3, 8)
        plt.plot(self.losses_history['G_loss'], label='Generator', alpha=0.7)
        plt.plot(self.losses_history['cycle_loss'], label='Cycle', alpha=0.7)
        plt.plot(self.losses_history['identity_loss'], label='Identity', alpha=0.7)
        plt.title('Core Losses Comparison')
        plt.xlabel('Epoch')
        plt.ylabel('Loss')
        plt.legend()
        plt.grid(True)
        
        plt.subplot(3, 3, 9)
        plt.plot(self.losses_history['structural_loss'], label='Structural', alpha=0.7, color='cyan')
        plt.plot(self.losses_history['dehazing_loss'], label='DCP Loss', alpha=0.7, color='magenta')
        plt.plot(self.losses_history['perceptual_loss'], label='Perceptual', alpha=0.7, color='brown')
        plt.title('å»éœ§å°ˆç”¨æå¤±å°æ¯” (Dehazing-Specific Losses)')
        plt.xlabel('Epoch')
        plt.ylabel('Loss')
        plt.legend()
        plt.grid(True)
        
        plt.tight_layout()
        plt.subplots_adjust(top=0.93)  # ç‚ºç¸½æ¨™é¡Œç•™ç©ºé–“
        
        # ç”Ÿæˆæ™‚é–“æˆ³é¿å…è¦†è“‹
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f'cyclegan_v6_optimized_losses_{timestamp}.png'
        plt.savefig(filename, dpi=300, bbox_inches='tight')
        plt.close()
        print(f"âœ… å„ªåŒ–ç‰ˆå»éœ§æå¤±å‡½æ•¸åœ–è¡¨å·²ä¿å­˜: {filename}")

def main():
    print("ğŸš€ å•Ÿå‹• CycleGAN v6.0 å»éœ§å°ˆç”¨è¨“ç·´")
    print("ä¸»è¦æ”¹é€²ï¼šçµæ§‹ä¿æŒæå¤± + å»éœ§å…ˆé©—æå¤± + é«˜æ¬Šé‡Identityæå¤±")
    print("ğŸ¯ é˜²æ­¢é¢¨æ ¼è½‰æ›åŒ–ï¼Œç”ŸæˆçœŸå¯¦å»éœ§æ•ˆæœ")
    
    # æ•¸æ“šåŠ è¼‰
    transform = transforms.Compose([
        transforms.Resize(256),
        transforms.RandomCrop(256),
        transforms.RandomHorizontalFlip(p=0.5),
        transforms.ToTensor(),
        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))
    ])
    
    dataset = CycleGANDataset(
        root_A='origin',  # æœ‰éœ§åœ–åƒ (ä½¿ç”¨å…¨éƒ¨1500å¼µ)
        root_B='clean',   # æ¸…æ™°åœ–åƒ (é™åˆ¶ç‚º1800å¼µ)
        transform=transform,
        max_images_B=1800  # åªé™åˆ¶æ¸…æ™°åœ–åƒæ•¸é‡ç‚º1800å¼µ
    )
    
    dataloader = DataLoader(dataset, batch_size=1, shuffle=True, num_workers=2)
    print(f"ğŸ“Š æ•¸æ“šé›†å¤§å°: {len(dataset)} å°åœ–åƒ")
    
    # å‰µå»ºè¨“ç·´å™¨ - å»éœ§å°ˆç”¨åƒæ•¸
    trainer = V6CycleGANTrainer(
        lambda_cycle=10.0,          # å¾ªç’°ä¸€è‡´æ€§æå¤±
        lambda_identity=20.0,       # èº«ä»½æå¤± (æå‡4å€é˜²æ­¢é¢¨æ ¼åŒ–)
        lambda_perceptual=1.0,      # æ„ŸçŸ¥æå¤±
        lambda_structural=5.0,      # çµæ§‹ä¿æŒæå¤± (é˜²æ­¢éåº¦è®Šå½¢)
        lambda_dehazing=3.0,        # å»éœ§å…ˆé©—æå¤± (æš—é€šé“+å°æ¯”åº¦)
        lr=0.0002
    )
    
    # è¨“ç·´åƒæ•¸
    num_epochs = 200
    start_time = datetime.now()
    
    print(f"ğŸƒ é–‹å§‹è¨“ç·´ {num_epochs} å€‹ epochs...")
    
    for epoch in range(1, num_epochs + 1):
        print(f"\n=== Epoch {epoch}/{num_epochs} ===")
        
        # è¨“ç·´ä¸€å€‹ epoch
        epoch_losses = trainer.train_epoch(dataloader, epoch)
        
        # è¨˜éŒ„æå¤±
        for key, value in epoch_losses.items():
            trainer.losses_history[key].append(value)
        
        # æ›´æ–°å­¸ç¿’ç‡
        trainer.scheduler_G.step()
        trainer.scheduler_D_A.step()
        trainer.scheduler_D_B.step()
        
        # æ‰“å°æå¤±
        print(f"æå¤± - G: {epoch_losses['G_loss']:.4f}, D_A: {epoch_losses['D_A_loss']:.4f}, "
              f"D_B: {epoch_losses['D_B_loss']:.4f}, Cycle: {epoch_losses['cycle_loss']:.4f}")
        print(f"      Identity: {epoch_losses['identity_loss']:.4f}, Perceptual: {epoch_losses['perceptual_loss']:.4f}, "
              f"Structural: {epoch_losses['structural_loss']:.4f}, DCP: {epoch_losses['dehazing_loss']:.4f}")
        
        # æ¯20å€‹epochä¿å­˜æª¢æŸ¥é»
        if epoch % 20 == 0:
            trainer.save_checkpoint(epoch)
        
        # æ¯50å€‹epochç¹ªè£½æå¤±æ›²ç·š
        if epoch % 50 == 0:
            trainer.plot_losses()
            print(f"ğŸ“Š Epoch {epoch} æå¤±åœ–è¡¨å·²ç”Ÿæˆ")
    
    # ä¿å­˜æœ€çµ‚æ¨¡å‹
    trainer.save_checkpoint(num_epochs)
    
    # æœ€çµ‚æå¤±åœ–
    trainer.plot_losses()
    
    end_time = datetime.now()
    training_time = end_time - start_time
    
    # ç”Ÿæˆæ™‚é–“æˆ³å’Œæ–‡ä»¶å
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    loss_chart_name = f'cyclegan_v6_optimized_losses_{timestamp}.png'
    
    print(f"\nğŸ‰ å„ªåŒ–ç‰ˆå»éœ§å°ˆç”¨ CycleGAN v6.0 è¨“ç·´å®Œæˆ!")
    print(f"â±ï¸ ç¸½è¨“ç·´æ™‚é–“: {training_time}")
    print(f"ğŸ’¾ æ¨¡å‹å·²ä¿å­˜: checkpoints/cyclegan_v6_epoch_{num_epochs}.pth")
    print(f"ğŸ“Š å„ªåŒ–ç‰ˆæå¤±åœ–è¡¨å·²ç”Ÿæˆ: {loss_chart_name}")
    print(f"ğŸ”¥ åŒ…å«å®Œæ•´çš„ DCP æå¤± (æš—é€šé“å…ˆé©—å»éœ§æå¤±) åˆ†æ")
    print(f"ğŸ¯ æ¨¡å‹ä½¿ç”¨ [0,1] åŸŸè¨ˆç®—ã€æœ€å°æ± åŒ–ã€åå°„paddingã€LRè¡°æ¸›åˆ°0")

if __name__ == "__main__":
    main()