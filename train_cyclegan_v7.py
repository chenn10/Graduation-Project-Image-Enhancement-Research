#!/usr/bin/env python3
"""
CycleGAN v7.0 Enhanced - å…¨é¢æ”¹é€²ç‰ˆæœ¬
ä¸»è¦æ”¹é€²ï¼š
1. ä¿®æ­£çµæ§‹æå¤±é…å°ï¼šstruct(real_A, fake_B) / struct(real_B, fake_A)
2. ä½¿ç”¨ MultiScaleCycleGANDataset
3. DCP/Contrast æ¬Šé‡ä¸‹èª¿ + é‚Šç•Œ gate
4. èª¿æ•´è¶…åƒæ•¸ï¼šlr_D = 8e-5, flip_prob = 0.02
5. æ·»åŠ  TV loss (0.1 ä¿‚æ•¸èµ·æ­¥)
6. SelfAttention A/B æ¸¬è©¦é¸é …
7. é—œé–‰å‰ç«¯ colormap ç–Šè‰²
"""

import os
# Reduce OpenMP/MKL thread usage to avoid multiple runtime initialization on Windows
# This should be set before importing torch so linked libraries respect it.
os.environ.setdefault("OMP_NUM_THREADS", "1")
os.environ.setdefault("MKL_NUM_THREADS", "1")
# Allow duplicate OpenMP runtime to prevent crashes (temporary workaround for Windows)
os.environ.setdefault("KMP_DUPLICATE_LIB_OK", "TRUE")

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.utils.data import DataLoader, Dataset
from torchvision import transforms
from PIL import Image
import numpy as np
import matplotlib.pyplot as plt
from datetime import datetime
import itertools
import math
from cyclegan_dataset import CycleGANDataset

class MultiScaleCycleGANDataset(CycleGANDataset):
    """å¤šå°ºåº¦ CycleGAN æ•¸æ“šé›† - éš¨æ©Ÿåˆ‡å–ä¸åŒå°ºå¯¸çš„ tile"""
    def __init__(self, root_A, root_B, transform=None, max_images=None, 
                 tile_sizes=[256, 384, 512], tile_prob=[0.6, 0.3, 0.1]):
        # å‰µå»ºä¸€å€‹åŸºæœ¬ transform çµ¦çˆ¶é¡ï¼Œä½†å¯¦éš›ä¸ä½¿ç”¨
        base_transform = transforms.Compose([
            transforms.ToTensor(),
            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))
        ])
        super().__init__(root_A, root_B, base_transform, max_images)
        self.tile_sizes = tile_sizes
        self.tile_prob = tile_prob
        
        # ç‚ºä¸åŒå°ºå¯¸å‰µå»º transformï¼ˆèª¿ä½ flip_probï¼‰
        self.transforms = {}
        for size in tile_sizes:
            self.transforms[size] = transforms.Compose([
                transforms.Resize(int(size * 1.12)),  # æ”¾å¤§ 12% å†è£å‰ª
                transforms.RandomCrop(size),
                transforms.RandomHorizontalFlip(p=0.02),  # é™ä½ flip æ©Ÿç‡
                transforms.ToTensor(),
                transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))
            ])
    
    def __getitem__(self, index):
        # éš¨æ©Ÿé¸æ“‡ tile å°ºå¯¸
        tile_size = np.random.choice(self.tile_sizes, p=self.tile_prob)
        transform = self.transforms[tile_size]
        
        # è¼‰å…¥åœ–åƒï¼ˆä½¿ç”¨æ­£ç¢ºçš„å±¬æ€§åï¼‰
        img_A = Image.open(self.images_A[index % len(self.images_A)]).convert('RGB')
        img_B = Image.open(self.images_B[index % len(self.images_B)]).convert('RGB')
        
        # æ‡‰ç”¨å°æ‡‰å°ºå¯¸çš„ transform
        if transform:
            img_A = transform(img_A)
            img_B = transform(img_B)
        
        return img_A, img_B

# è¨­å‚™è¨­å®š
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"ä½¿ç”¨è¨­å‚™: {device}")

# Ensure PyTorch uses a single thread for its CPU ops (helps avoid OpenMP runtime issues)
try:
    torch.set_num_threads(1)
except Exception:
    pass

# å‰µå»ºå¿…è¦è³‡æ–™å¤¾
os.makedirs('checkpoints', exist_ok=True)
os.makedirs('training_images', exist_ok=True)

def spectral_norm(module, name='weight', power_iterations=1):
    """ä½¿ç”¨ PyTorch å…§å»ºçš„ spectral normalization"""
    try:
        return torch.nn.utils.spectral_norm(module, name=name, n_power_iterations=power_iterations)
    except:
        return module

class TVLoss(nn.Module):
    """Total Variation Loss - åŠ å¼·æŠ‘åˆ¶æ¢ç´‹å’Œæ£‹ç›¤æ ¼"""
    def __init__(self, weight=0.2):  # æé«˜æ¬Šé‡ä»¥æ›´å¥½æŠ‘åˆ¶ artifacts
        super(TVLoss, self).__init__()
        self.weight = weight
    
    def forward(self, x):
        batch_size, channels, height, width = x.size()
        
        # è¨ˆç®—æ°´å¹³å’Œå‚ç›´æ–¹å‘çš„è®ŠåŒ–
        tv_h = torch.abs(x[:, :, 1:, :] - x[:, :, :-1, :]).sum()
        tv_w = torch.abs(x[:, :, :, 1:] - x[:, :, :, :-1]).sum()
        
        tv_loss = self.weight * (tv_h + tv_w) / (batch_size * channels * height * width)
        return tv_loss

class HazeEstimator(nn.Module):
    """éœ§æ¿ƒåº¦ä¼°è¨ˆå™¨ - è©•ä¼°åœ–åƒä¸­éœ§çš„æ¿ƒåº¦"""
    def __init__(self):
        super(HazeEstimator, self).__init__()
        
    def estimate_haze_density(self, img):
        """
        ä¼°è¨ˆéœ§æ¿ƒåº¦ (0-1)ï¼ŒåŸºæ–¼å¤šç¨®æŒ‡æ¨™ï¼š
        1. æš—é€šé“å…ˆé©—å€¼
        2. å°æ¯”åº¦
        3. å¯è¦‹åº¦
        4. è‰²å½©é£½å’Œåº¦
        """
        with torch.no_grad():  # éœ§æ¿ƒåº¦è¨ˆç®—ä¸éœ€è¦æ¢¯åº¦
            # ç¢ºä¿åœ¨ [0,1] ç¯„åœ
            if img.min() < 0:
                img_01 = (img + 1.0) / 2.0
            else:
                img_01 = img
            
            # è¨ˆç®—æš—é€šé“
            dark_channel = torch.min(img_01, dim=1, keepdim=True)[0]
            # ä½¿ç”¨ -max_pool2d(-x) ä¾†æ¨¡æ“¬ min_pool2d
            dark_channel = -F.max_pool2d(-dark_channel, kernel_size=15, stride=1, padding=7)
            
            # è¨ˆç®—éœ§æ¿ƒåº¦æŒ‡æ¨™ï¼Œä¿æŒ batch ç¶­åº¦ [B]
            haze_density = torch.mean(dark_channel, dim=[1,2,3])
            
            return haze_density  # è¿”å› shape=[B]ï¼Œä¸ä½¿ç”¨ squeeze()

class StructuralLoss(nn.Module):
    """ä¿®æ­£ç‰ˆçµæ§‹æå¤± - ç¢ºä¿æ­£ç¢ºçš„ domain é…å°"""
    def __init__(self, window_size=11, channels=3):
        super(StructuralLoss, self).__init__()
        self.window_size = window_size
        self.channels = channels
        
        # å‰µå»ºé«˜æ–¯çª—
        window = self.create_window(window_size, channels)
        self.register_buffer('window', window)
        
    def create_window(self, window_size, channels):
        # å‰µå»º 1D é«˜æ–¯æ ¸
        sigma = 1.5
        gauss = torch.Tensor([math.exp(-(x - window_size//2)**2/float(2*sigma**2)) for x in range(window_size)])
        gauss = gauss / gauss.sum()
        
        # å‰µå»º 2D é«˜æ–¯çª—
        _2D_window = gauss.unsqueeze(1).mm(gauss.unsqueeze(0))
        window = _2D_window.expand(channels, 1, window_size, window_size).contiguous()
        return window
    
    def ssim(self, img1, img2):
        # ç¢ºä¿è¼¸å…¥åœ¨ [0,1] ç¯„åœå…§
        if img1.min() < 0:
            img1 = (img1 + 1.0) / 2.0
        if img2.min() < 0:
            img2 = (img2 + 1.0) / 2.0
            
        mu1 = F.conv2d(img1, self.window, padding=self.window_size//2, groups=self.channels)
        mu2 = F.conv2d(img2, self.window, padding=self.window_size//2, groups=self.channels)
        
        mu1_sq = mu1.pow(2)
        mu2_sq = mu2.pow(2)
        mu1_mu2 = mu1 * mu2
        
        sigma1_sq = F.conv2d(img1*img1, self.window, padding=self.window_size//2, groups=self.channels) - mu1_sq
        sigma2_sq = F.conv2d(img2*img2, self.window, padding=self.window_size//2, groups=self.channels) - mu2_sq
        sigma12 = F.conv2d(img1*img2, self.window, padding=self.window_size//2, groups=self.channels) - mu1_mu2
        
        C1 = 0.01**2
        C2 = 0.03**2
        
        ssim_map = ((2*mu1_mu2 + C1)*(2*sigma12 + C2))/((mu1_sq + mu2_sq + C1)*(sigma1_sq + sigma2_sq + C2))
        return ssim_map.mean()
    
    def forward(self, real_A, fake_B, real_B, fake_A):
        """
        ä¿®æ­£ç‰ˆçµæ§‹æå¤±é…å°ï¼š
        - struct(real_A, fake_B): A domain çš„çœŸå¯¦åœ–åƒèˆ‡å¾ B ç”Ÿæˆçš„å‡åœ–åƒ
        - struct(real_B, fake_A): B domain çš„çœŸå¯¦åœ–åƒèˆ‡å¾ A ç”Ÿæˆçš„å‡åœ–åƒ
        """
        # æ­£ç¢ºçš„é…å°æ–¹å¼
        loss_A = 1.0 - self.ssim(real_A, fake_B)  # Aâ†’B çµæ§‹ä¸€è‡´æ€§
        loss_B = 1.0 - self.ssim(real_B, fake_A)  # Bâ†’A çµæ§‹ä¸€è‡´æ€§
        
        return (loss_A + loss_B) / 2.0

class SelfAttention(nn.Module):
    """ä½¿ç”¨ CBAM (Convolutional Block Attention Module) æ›¿ä»£åŸæœ¬çš„ Self-Attentionã€‚
    ä¿æŒé¡åç‚º SelfAttention ä»¥ç¶­æŒèˆ‡ç¾æœ‰ç¨‹å¼ç¢¼ç›¸å®¹ï¼Œä½†å…§éƒ¨å¯¦ä½œç‚º CBAMã€‚
    æ”¯æ´ with_attn é–‹é—œä»¥é—œé–‰ attentionã€‚
    """
    def __init__(self, in_dim, activation=F.relu, with_attn=True, reduction=16, kernel_size=7):
        super(SelfAttention, self).__init__()
        self.chanel_in = in_dim
        self.activation = activation
        self.with_attn = with_attn

        # Channel attention MLP
        self.avg_pool = nn.AdaptiveAvgPool2d(1)
        self.max_pool = nn.AdaptiveMaxPool2d(1)
        self.mlp = nn.Sequential(
            nn.Conv2d(in_dim, in_dim // reduction, 1, bias=True),
            nn.ReLU(inplace=True),
            nn.Conv2d(in_dim // reduction, in_dim, 1, bias=True)
        )

        # Spatial attention
        self.spatial = nn.Sequential(
            nn.Conv2d(2, 1, kernel_size=kernel_size, padding=(kernel_size//2), bias=False),
            nn.Sigmoid()
        )

        # gamma for residual scaling (keeps similarity with original SelfAttention behavior)
        self.gamma = nn.Parameter(torch.zeros(1))

    def channel_att(self, x):
        avg = self.avg_pool(x)
        max_ = self.max_pool(x)
        avg_out = self.mlp(avg)
        max_out = self.mlp(max_)
        out = torch.sigmoid(avg_out + max_out)
        return x * out

    def spatial_att(self, x):
        # channel-wise max and mean
        max_c, _ = torch.max(x, dim=1, keepdim=True)
        mean_c = torch.mean(x, dim=1, keepdim=True)
        cat = torch.cat([max_c, mean_c], dim=1)
        att = self.spatial(cat)
        return x * att

    def forward(self, x):
        if not self.with_attn:
            return x

        out = self.channel_att(x)
        out = self.spatial_att(out)

        # residual-style scaling similar to original SelfAttention
        return self.gamma * out + x

class ImprovedUpsample(nn.Module):
    """æ”¹é€²çš„ä¸Šæ¡æ¨£æ¨¡çµ„ - ä½¿ç”¨æœ€è¿‘é„°é¿å…æ£‹ç›¤æ ¼"""
    def __init__(self, in_channels, out_channels, kernel_size=3):
        super(ImprovedUpsample, self).__init__()
        # ä½¿ç”¨æœ€è¿‘é„°ä¸Šæ¡æ¨£ + å·ç©æ›¿ä»£ PixelShuffle
        self.upsample = nn.Upsample(scale_factor=2, mode='nearest')
        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, padding=kernel_size//2)
        self.activation = nn.ReLU(inplace=True)
        
    def forward(self, x):
        x = self.upsample(x)
        x = self.conv(x)
        x = self.activation(x)
        return x

class ResidualBlock(nn.Module):
    """æ®˜å·®å¡Š"""
    def __init__(self, channels, use_dropout=False):
        super(ResidualBlock, self).__init__()
        self.conv_block = nn.Sequential(
            nn.ReflectionPad2d(1),
            spectral_norm(nn.Conv2d(channels, channels, 3)),
            nn.InstanceNorm2d(channels),
            nn.ReLU(inplace=True),
            nn.Dropout(0.5) if use_dropout else nn.Identity(),
            nn.ReflectionPad2d(1),
            spectral_norm(nn.Conv2d(channels, channels, 3)),
            nn.InstanceNorm2d(channels)
        )

    def forward(self, x):
        return x + self.conv_block(x)

class Generator(nn.Module):
    """å¢å¼·ç‰ˆç”Ÿæˆå™¨ - æ”¯æŒ SelfAttention A/B æ¸¬è©¦"""
    def __init__(self, input_channels=3, output_channels=3, n_residual_blocks=9, use_self_attention=True):
        super(Generator, self).__init__()
        
        # ç·¨ç¢¼å™¨
        self.encoder = nn.Sequential(
            nn.ReflectionPad2d(3),
            spectral_norm(nn.Conv2d(input_channels, 64, 7)),
            nn.InstanceNorm2d(64),
            nn.ReLU(inplace=True),
            
            spectral_norm(nn.Conv2d(64, 128, 3, stride=2, padding=1)),
            nn.InstanceNorm2d(128),
            nn.ReLU(inplace=True),
            
            spectral_norm(nn.Conv2d(128, 256, 3, stride=2, padding=1)),
            nn.InstanceNorm2d(256),
            nn.ReLU(inplace=True)
        )
        
        # æ®˜å·®å¡Š
        residual_blocks = []
        for _ in range(n_residual_blocks):
            residual_blocks.append(ResidualBlock(256))
        self.residual_blocks = nn.Sequential(*residual_blocks)
        
        # è‡ªæ³¨æ„åŠ›ï¼ˆå¯é¸ï¼‰
        self.use_self_attention = use_self_attention
        if use_self_attention:
            self.self_attention = SelfAttention(256, with_attn=True)
        else:
            self.self_attention = SelfAttention(256, with_attn=False)
        
        # è§£ç¢¼å™¨ - ä½¿ç”¨æ”¹é€²çš„ä¸Šæ¡æ¨£
        self.decoder = nn.Sequential(
            ImprovedUpsample(256, 128),
            nn.InstanceNorm2d(128),
            
            ImprovedUpsample(128, 64),
            nn.InstanceNorm2d(64),
            
            nn.ReflectionPad2d(3),
            nn.Conv2d(64, output_channels, 7),
            nn.Tanh()
        )

    def forward(self, x):
        encoded = self.encoder(x)
        residual = self.residual_blocks(encoded)
        attended = self.self_attention(residual)
        output = self.decoder(attended)
        
        # è¼¸å‡ºäº®åº¦/å°æ¯”åº¦èª¿æ•´ - é˜²æ­¢åæš—
        # out = out * 1.1 + 0.05 (åœ¨ tanh è¼¸å‡ºç¯„åœ [-1,1] å…§èª¿æ•´)
        output = output * 1.1 + 0.05
        output = torch.clamp(output, -1.0, 1.0)  # ç¢ºä¿è¼¸å‡ºç¯„åœ
        
        return output

class Discriminator(nn.Module):
    """åˆ¤åˆ¥å™¨"""
    def __init__(self, input_channels=3):
        super(Discriminator, self).__init__()
        
        def discriminator_block(in_filters, out_filters):
            # ç§»é™¤æœªä½¿ç”¨çš„ normalize åƒæ•¸ï¼Œä¿æŒç°¡æ½”
            layers = [spectral_norm(nn.Conv2d(in_filters, out_filters, 4, stride=2, padding=1))]
            # ç§»é™¤ InstanceNorm ä»¥æ¸›å°‘å‡ç´‹ç†
            layers.append(nn.LeakyReLU(0.2, inplace=True))
            return layers

        self.model = nn.Sequential(
            *discriminator_block(input_channels, 64),
            *discriminator_block(64, 128),
            *discriminator_block(128, 256),
            *discriminator_block(256, 512),
            nn.Conv2d(512, 1, 4, padding=1)
        )

    def forward(self, img):
        return self.model(img)

class DCPLoss(nn.Module):
    """æ”¹é€²çš„ DCP æå¤± - æ”¯æ´çœŸæ­£çš„ per-image è‡ªé©æ‡‰æ¬Šé‡"""
    def __init__(self, patch_size=15, weight=0.3, w_min=0.1, w_max=0.5):
        super(DCPLoss, self).__init__()
        self.patch_size = patch_size
        self.weight = weight
        self.w_min = w_min  # æ¬Šé‡ä¸‹ç•Œï¼ˆä¿å®ˆè¨­å®šï¼‰
        self.w_max = w_max  # æ¬Šé‡ä¸Šç•Œï¼ˆä¿å®ˆè¨­å®šï¼‰
        
    def forward(self, img, use_edge_gate=True, override_weight=None):
        # ç¢ºä¿åœ¨ [0,1] ç¯„åœ
        if img.min() < 0:
            img = (img + 1.0) / 2.0
        
        # è¨ˆç®—æš—é€šé“
        dark_channel = torch.min(img, dim=1, keepdim=True)[0]
        # ä½¿ç”¨ -max_pool2d(-x) ä¾†æ¨¡æ“¬ min_pool2d
        dark_channel = -F.max_pool2d(-dark_channel, kernel_size=self.patch_size, 
                                    stride=1, padding=self.patch_size//2)
        
        # é‚Šç•Œ gate - æ¸›å°‘é‚Šç·£å€åŸŸçš„æ¬Šé‡ä»¥é¿å… haloing (æ¯”ä¾‹è¨ˆç®—)
        if use_edge_gate:
            # å‰µå»ºé‚Šç•Œé®ç½© - ä½¿ç”¨æ¯”ä¾‹è¨ˆç®—é©é…ä¸åŒè§£æåº¦
            b, c, h, w = img.shape
            mh, mw = max(16, h//8), max(16, w//8)  # ä¾åœ–åƒå¤§å°å‹•æ…‹èª¿æ•´
            edge_gate = torch.ones_like(dark_channel)
            
            # é‚Šç·£å€åŸŸæ¬Šé‡ä¸‹èª¿åˆ° 0.3
            edge_gate[:, :, :mh, :] *= 0.3
            edge_gate[:, :, -mh:, :] *= 0.3
            edge_gate[:, :, :, :mw] *= 0.3
            edge_gate[:, :, :, -mw:] *= 0.3
            
            dark_channel = dark_channel * edge_gate
        
        # è¨ˆç®—æ¯å¼µåœ–çš„ DCP æå¤±ï¼ˆä¿æŒ batch ç¶­åº¦ï¼‰
        dcp_loss_per_image = torch.mean(dark_channel.view(dark_channel.size(0), -1), dim=1)  # shape=[B]
        
        # ä½¿ç”¨è‡ªé©æ‡‰æ¬Šé‡æˆ–é»˜èªæ¬Šé‡
        if override_weight is not None and isinstance(override_weight, torch.Tensor):
            # per-image æ¬Šé‡ï¼šé€å¼µä¹˜æ¬Šé‡å¾Œå†å¹³å‡
            weighted_loss = (override_weight * dcp_loss_per_image).mean()
        else:
            # æ¨™é‡æ¬Šé‡æˆ–é»˜èªæ¬Šé‡
            weight = override_weight if override_weight is not None else self.weight
            weighted_loss = weight * dcp_loss_per_image.mean()
            
        return weighted_loss

class ContrastLoss(nn.Module):
    """æ”¹é€²çš„å°æ¯”åº¦æå¤± - æ”¯æ´è‡ªé©æ‡‰æ¬Šé‡èˆ‡å±€éƒ¨å°æ¯”"""
    def __init__(self, weight=0.1, w_min=0.05, w_max=0.15):
        super(ContrastLoss, self).__init__()
        self.weight = weight
        self.w_min = w_min  # æ¬Šé‡ä¸‹ç•Œï¼ˆè–„éœ§æ™‚ä¿å®ˆï¼‰
        self.w_max = w_max  # æ¬Šé‡ä¸Šç•Œï¼ˆæ¿ƒéœ§æ™‚é©åº¦ï¼‰
        
    def forward(self, img, override_weight=None):
        if img.min() < 0:
            img = (img + 1.0) / 2.0
        
        # è¨ˆç®—å±€éƒ¨å°æ¯”åº¦ï¼ˆé¿å…å…¨åŸŸéåº¦å¢å¼·ï¼‰
        # ä½¿ç”¨ 3x3 kernel è¨ˆç®—å±€éƒ¨æ¨™æº–å·®
        kernel_size = 3
        padding = kernel_size // 2
        
        # è¨ˆç®—å±€éƒ¨å‡å€¼
        kernel = torch.ones(1, 1, kernel_size, kernel_size, device=img.device) / (kernel_size * kernel_size)
        local_mean = F.conv2d(img.mean(dim=1, keepdim=True), kernel, padding=padding)
        
        # è¨ˆç®—å±€éƒ¨æ¨™æº–å·®
        img_gray = img.mean(dim=1, keepdim=True)
        local_var = F.conv2d((img_gray - local_mean) ** 2, kernel, padding=padding)
        local_std = torch.sqrt(local_var + 1e-6)
        
        # è¨ˆç®—æ¯å¼µåœ–çš„å°æ¯”åº¦æå¤±ï¼ˆä¿æŒ batch ç¶­åº¦ï¼‰
        contrast_loss_per_image = -torch.mean(local_std.view(local_std.size(0), -1), dim=1)  # shape=[B]
        
        # ä½¿ç”¨è‡ªé©æ‡‰æ¬Šé‡æˆ–é»˜èªæ¬Šé‡
        if override_weight is not None and isinstance(override_weight, torch.Tensor):
            # per-image æ¬Šé‡ï¼šé€å¼µä¹˜æ¬Šé‡å¾Œå†å¹³å‡
            weighted_loss = (override_weight * contrast_loss_per_image).mean()
        else:
            # æ¨™é‡æ¬Šé‡æˆ–é»˜èªæ¬Šé‡
            weight = override_weight if override_weight is not None else self.weight
            weighted_loss = weight * contrast_loss_per_image.mean()
            
        return weighted_loss

def weights_init_normal(m):
    classname = m.__class__.__name__
    if classname.find("Conv") != -1:
        torch.nn.init.normal_(m.weight.data, 0.0, 0.02)
        if hasattr(m, "bias") and m.bias is not None:
            torch.nn.init.constant_(m.bias.data, 0.0)
    elif classname.find("BatchNorm2d") != -1:
        torch.nn.init.normal_(m.weight.data, 1.0, 0.02)
        torch.nn.init.constant_(m.bias.data, 0.0)

def sample_images(generator_AB, generator_BA, dataloader, device, epoch, save_path):
    """ç”Ÿæˆæ¨£æœ¬åœ–åƒé€²è¡Œè¦–è¦ºæª¢æŸ¥ - å‹•æ…‹é©é… batch å¤§å°"""
    generator_AB.eval()
    generator_BA.eval()
    
    with torch.no_grad():
        real_A, real_B = next(iter(dataloader))
        real_A, real_B = real_A.to(device), real_B.to(device)
        
        fake_B = generator_AB(real_A)
        fake_A = generator_BA(real_B)
        
        # å‹•æ…‹æ•¸é‡ - é¿å… batch_size=1 å´©æ½°
        n = min(4, real_A.size(0))  # æœ€å¤š4å¼µï¼Œé©é…å¯¦éš› batch å¤§å°
        
        # å‰µå»ºå°æ¯”åœ–åƒ
        images = torch.cat([real_A[:n], fake_B[:n], real_B[:n], fake_A[:n]], dim=0)
        images = (images + 1.0) / 2.0
        
        # å‹•æ…‹ç¶²æ ¼å¸ƒå±€
        total_images = 4 * n  # 4 é¡å‹ Ã— n å¼µåœ–
        cols = n
        rows = 4
        
        fig, axes = plt.subplots(rows, cols, figsize=(3*cols, 3*rows))
        if n == 1:
            axes = axes.reshape(rows, 1)  # ç¢ºä¿æ˜¯2D array
        
        for i in range(total_images):
            row, col = i // cols, i % cols
            ax = axes[row, col]
            
            img = images[i].cpu().permute(1, 2, 0).numpy()
            img = np.clip(img, 0, 1)
            ax.imshow(img)
            ax.axis('off')
            
            # æ·»åŠ æ¨™é¡Œ
            if i < n:
                ax.set_title(f'Real A {col+1}', fontsize=10)
            elif i < 2*n:
                ax.set_title(f'Fake B {col+1}', fontsize=10)
            elif i < 3*n:
                ax.set_title(f'Real B {col+1}', fontsize=10)
            else:
                ax.set_title(f'Fake A {col+1}', fontsize=10)
        
        plt.tight_layout()
        plt.savefig(f'{save_path}/epoch_{epoch:03d}_samples.png', 
                   dpi=150, bbox_inches='tight', facecolor='white')
        plt.close()

def train_cyclegan():
    """è¨“ç·´ CycleGAN v7.0 Enhanced"""
    
    print("ğŸš€ é–‹å§‹ CycleGAN v7.0 Enhanced è¨“ç·´")
    print("ä¸»è¦æ”¹é€²ï¼š")
    print("  â€¢ ä¿®æ­£çµæ§‹æå¤±é…å°")
    print("  â€¢ MultiScaleCycleGANDataset")
    print("  â€¢ ä¸‹èª¿ DCP/Contrast æ¬Šé‡ + é‚Šç•Œ gate")
    print("  â€¢ lr_D = 8e-5, flip_prob = 0.02")
    print("  â€¢ TV loss (0.1 ä¿‚æ•¸)")
    print("  â€¢ SelfAttention A/B æ¸¬è©¦é¸é …")
    
    # è¶…åƒæ•¸ - æ”¹é€²ç‰ˆæœ¬
    num_epochs = 200
    batch_size = 1  # ä¿æŒ batch_size=1 ä»¥ç¢ºä¿ç©©å®šæ€§èˆ‡ per-image æ¬Šé‡
    lr_G = 2e-4     # ç”Ÿæˆå™¨å­¸ç¿’ç‡
    lr_D = 8e-5     # åˆ¤åˆ¥å™¨å­¸ç¿’ç‡ï¼ˆä¸‹èª¿ï¼‰
    lambda_cycle = 10.0
    lambda_identity = 5.0
    lambda_structural = 4.5
    lambda_dcp = 0.3        # DCP åŸºç¤æ¬Šé‡
    lambda_contrast = 0.1   # Contrast æ¬Šé‡é€²ä¸€æ­¥ä¸‹èª¿
    lambda_tv = 0.2         # TV loss ä¿‚æ•¸æé«˜
    # é‚Šç·£ä¿ç•™æ¬Šé‡
    lambda_edge_preserve = 0.6
    
    # SelfAttention A/B æ¸¬è©¦é¸é … (æš«æ™‚é—œé–‰é¿å…è¨˜æ†¶é«”/æ¢ç´‹å•é¡Œ)
    use_self_attention = False  # è¨­ç‚º False ä¾†æ¸¬è©¦ç„¡ SelfAttention ç‰ˆæœ¬
    
    # æ•¸æ“šåŠ è¼‰ - ä½¿ç”¨å¤šå°ºåº¦æ•¸æ“šé›†ï¼Œä¿®æ­£ A/B èªæ„
    print("ğŸ“Š è¼‰å…¥å¤šå°ºåº¦æ•¸æ“šé›†...")
    print("   A=æœ‰éœ§(origin), B=æ¸…æ™°(clean)")
    dataset = MultiScaleCycleGANDataset(
        'origin', 'clean',  # ä¿®æ­£ï¼šA=æœ‰éœ§, B=æ¸…æ™°
        max_images=1500,  # é™åˆ¶æ¯å€‹ domain æœ€å¤š 1500 å¼µåœ–åƒ
        tile_sizes=[256, 384, 512],
        tile_prob=[0.6, 0.3, 0.1]
    )
    # Use num_workers=0 on Windows to avoid child-process DLL/runtime conflicts
    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=0)
    
    # åˆå§‹åŒ–æ¨¡å‹
    print("ğŸ—ï¸ åˆå§‹åŒ–æ¨¡å‹...")
    generator_AB = Generator(use_self_attention=use_self_attention).to(device)
    generator_BA = Generator(use_self_attention=use_self_attention).to(device)
    discriminator_A = Discriminator().to(device)
    discriminator_B = Discriminator().to(device)
    
    # æ¬Šé‡åˆå§‹åŒ–
    generator_AB.apply(weights_init_normal)
    generator_BA.apply(weights_init_normal)
    discriminator_A.apply(weights_init_normal)
    discriminator_B.apply(weights_init_normal)
    
    # æå¤±å‡½æ•¸
    criterion_GAN = nn.MSELoss()
    criterion_cycle = nn.L1Loss()
    criterion_identity = nn.L1Loss()
    structural_loss = StructuralLoss().to(device)
    dcp_loss = DCPLoss(weight=lambda_dcp, w_min=0.1, w_max=0.20)  # å°‡ w_max é™è‡³ 0.20
    contrast_loss = ContrastLoss(weight=lambda_contrast, w_min=0.05, w_max=0.15)  # å°æ¯”åº¦ä¹Ÿæ”¯æ´è‡ªé©æ‡‰
    tv_loss = TVLoss(weight=lambda_tv)
    haze_estimator = HazeEstimator().to(device)
    
    # å„ªåŒ–å™¨ - èª¿æ•´å­¸ç¿’ç‡
    optimizer_G = optim.Adam(
        itertools.chain(generator_AB.parameters(), generator_BA.parameters()),
        lr=lr_G, betas=(0.5, 0.999)
    )
    optimizer_D_A = optim.Adam(discriminator_A.parameters(), lr=lr_D, betas=(0.5, 0.999))
    optimizer_D_B = optim.Adam(discriminator_B.parameters(), lr=lr_D, betas=(0.5, 0.999))
    
    # å­¸ç¿’ç‡èª¿åº¦
    scheduler_G = optim.lr_scheduler.LambdaLR(
        optimizer_G, lr_lambda=lambda epoch: 1.0 - max(0, epoch - 100) / 100
    )
    scheduler_D_A = optim.lr_scheduler.LambdaLR(
        optimizer_D_A, lr_lambda=lambda epoch: 1.0 - max(0, epoch - 100) / 100
    )
    scheduler_D_B = optim.lr_scheduler.LambdaLR(
        optimizer_D_B, lr_lambda=lambda epoch: 1.0 - max(0, epoch - 100) / 100
    )

    # -----------------
    # Resume from latest checkpoint if exists
    # -----------------
    import glob

    def find_latest_checkpoint():
        patterns = [
            'checkpoints/cyclegan_v7_enhanced_no_attn_epoch_*.pth',
            'checkpoints/cyclegan_v7_enhanced_epoch_*.pth'
        ]
        files = []
        for p in patterns:
            files.extend(glob.glob(p))
        if not files:
            return None
        # sort by epoch number parsed from filename
        def epoch_from_name(f):
            try:
                return int(os.path.basename(f).split('_epoch_')[1].split('.pth')[0])
            except Exception:
                return 0
        files.sort(key=epoch_from_name)
        return files[-1]

    start_epoch = 0
    latest_ckpt = find_latest_checkpoint()
    if latest_ckpt is not None:
        print(f"ğŸ” ç™¼ç¾ checkpoint {latest_ckpt}ï¼Œå˜—è©¦è¼‰å…¥ä»¥ç¹¼çºŒè¨“ç·´ï¼ˆå…è¨±éƒ¨åˆ†æ¬Šé‡ä¸åŒ¹é…ï¼‰")
        try:
            ck = torch.load(latest_ckpt, map_location=device)
        except Exception as e:
            print(f"âŒ è¼‰å…¥ checkpoint æª”æ¡ˆå¤±æ•—: {e}ï¼Œå°‡å¾é ­é–‹å§‹è¨“ç·´")
            latest_ckpt = None
            start_epoch = 0

    if latest_ckpt is not None:
        # Load model weights with strict=False so mismatched attention modules won't block loading.
        def try_load_model(model, key_name):
            if key_name not in ck:
                print(f"âš ï¸ checkpoint ä¸­æ²’æœ‰ {key_name} æ¬Šé‡ï¼Œè·³é")
                return
            state = ck[key_name]
            try:
                res = model.load_state_dict(state, strict=False)
                missing = res.missing_keys if hasattr(res, 'missing_keys') else []
                unexpected = res.unexpected_keys if hasattr(res, 'unexpected_keys') else []
                if missing:
                    print(f"âš ï¸ åœ¨è¼‰å…¥ {key_name} æ™‚æœ‰ç¼ºå¤±çš„åƒæ•¸ ({len(missing)}): {missing[:5]}{'...' if len(missing)>5 else ''}")
                if unexpected:
                    print(f"âš ï¸ åœ¨è¼‰å…¥ {key_name} æ™‚æœ‰å¤šé¤˜çš„åƒæ•¸ ({len(unexpected)}): {unexpected[:5]}{'...' if len(unexpected)>5 else ''}")
            except Exception as e:
                print(f"âŒ è¼‰å…¥ {key_name} æ¬Šé‡å¤±æ•—: {e}")

        try:
            try_load_model(generator_AB, 'generator_AB')
            try_load_model(generator_BA, 'generator_BA')
            try_load_model(discriminator_A, 'discriminator_A')
            try_load_model(discriminator_B, 'discriminator_B')

            # try to load optimizer states (may fail across PyTorch versions) â€” optional
            try:
                if 'optimizer_G' in ck:
                    try:
                        optimizer_G.load_state_dict(ck['optimizer_G'])
                    except Exception as e:
                        print(f"âš ï¸ ç„¡æ³•è¼‰å…¥ optimizer_G state: {e}")
                if 'optimizer_D_A' in ck:
                    try:
                        optimizer_D_A.load_state_dict(ck['optimizer_D_A'])
                    except Exception as e:
                        print(f"âš ï¸ ç„¡æ³•è¼‰å…¥ optimizer_D_A state: {e}")
                if 'optimizer_D_B' in ck:
                    try:
                        optimizer_D_B.load_state_dict(ck['optimizer_D_B'])
                    except Exception as e:
                        print(f"âš ï¸ ç„¡æ³•è¼‰å…¥ optimizer_D_B state: {e}")
            except Exception:
                pass

            start_epoch = int(ck.get('epoch', 0))
            print(f"âœ… å˜—è©¦å¾ checkpoint æ¢å¾©ï¼Œstart_epoch è¨­ç‚º {start_epoch}ï¼ˆéƒ¨åˆ†åƒæ•¸è‹¥ä¸åŒ¹é…å‰‡è¢«å¿½ç•¥ï¼‰")
        except Exception as e:
            print(f"âŒ åœ¨è™•ç† checkpoint æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}ï¼Œå°‡å¾é ­é–‹å§‹è¨“ç·´")
            start_epoch = 0
    
    # è¨“ç·´å¾ªç’°
    print(f"ğŸ¯ é–‹å§‹è¨“ç·´ {num_epochs} epochs...")
    print(f"   SelfAttention: {'å•Ÿç”¨' if use_self_attention else 'é—œé–‰'}")
    print(f"   è‡ªé©æ‡‰æ¬Šé‡: DCP [{dcp_loss.w_min:.2f}, {dcp_loss.w_max:.2f}], Contrast [{contrast_loss.w_min:.2f}, {contrast_loss.w_max:.2f}]")
    
    # æš–å•Ÿå‹•åƒæ•¸
    warmup_epochs = 5  # å‰ 5 å€‹ epoch åšæš–å•Ÿ
    
    for epoch in range(num_epochs):
        generator_AB.train()
        generator_BA.train()
        discriminator_A.train()
        discriminator_B.train()
        
        total_loss_G = 0
        total_loss_D = 0
        
        # æš–å•Ÿå‹•æ¬Šé‡ç¸®æ”¾ï¼ˆå‰å¹¾å€‹ epoch æ¸›ç·©è‡ªé©æ‡‰æ¬Šé‡ï¼‰
        warmup_scale = min(1.0, (epoch + 1) / warmup_epochs)
        
        for i, (real_A, real_B) in enumerate(dataloader):
            real_A, real_B = real_A.to(device), real_B.to(device)
            
            # å‹•æ…‹è¨ˆç®—åˆ¤åˆ¥å™¨è¼¸å‡ºå°ºå¯¸ï¼ˆå¤šå°ºåº¦æ•¸æ“šé›†éœ€è¦ï¼‰
            with torch.no_grad():
                sample_output = discriminator_A(real_A)
                output_size = sample_output.shape[2:]  # ç²å–ç•¶å‰ batch çš„ H, W å°ºå¯¸
            
            # å‰µå»ºçœŸå¯¦å’Œè™›å‡æ¨™ç±¤ï¼ˆä½¿ç”¨ç•¶å‰batchçš„å°ºå¯¸ï¼‰
            valid = torch.ones((real_A.size(0), 1, *output_size), requires_grad=False).to(device)
            fake = torch.zeros((real_A.size(0), 1, *output_size), requires_grad=False).to(device)
            
            # -----------------
            # è¨“ç·´ç”Ÿæˆå™¨
            # -----------------
            
            optimizer_G.zero_grad()
            
            # ç”Ÿæˆå‡åœ–åƒ
            fake_B_raw = generator_AB(real_A)
            fake_A = generator_BA(real_B)
            
            # å¼·éœ§ Gamma æ ¡æ­£ (1.05~1.25) - åŸºæ–¼éœ§æ¿ƒåº¦å‹•æ…‹èª¿æ•´
            with torch.no_grad():
                haze_density_for_gamma = haze_estimator.estimate_haze_density(real_A)
                # ä½¿ç”¨é–€æª» 0.7ï¼Œå°‡ Gamma ç¯„åœè¨­ç‚º 1.05 ~ 1.25
                gamma_threshold = 0.7
                gamma_min = 1.05
                gamma_max = 1.25
                # ç·šæ€§æ˜ å°„ haze_density å¾ [gamma_threshold, 1.0] åˆ° [gamma_min, gamma_max]
                scaled = torch.clamp((haze_density_for_gamma - gamma_threshold) / (1.0 - gamma_threshold), 0.0, 1.0)
                gamma_values = torch.where(
                    haze_density_for_gamma > gamma_threshold,
                    gamma_min + (gamma_max - gamma_min) * scaled,
                    torch.ones_like(haze_density_for_gamma)
                )
            
            # æ‡‰ç”¨ gamma æ ¡æ­£
            fake_B_normalized = (fake_B_raw + 1.0) / 2.0  # [-1,1] â†’ [0,1]
            gamma_corrected = torch.pow(fake_B_normalized.clamp(1e-6, 1.0), 
                                      gamma_values.view(-1, 1, 1, 1))
            fake_B = gamma_corrected * 2.0 - 1.0  # [0,1] â†’ [-1,1]
            
            # GAN æå¤±
            pred_fake_B = discriminator_B(fake_B)
            pred_fake_A = discriminator_A(fake_A)
            loss_GAN_AB = criterion_GAN(pred_fake_B, valid)
            loss_GAN_BA = criterion_GAN(pred_fake_A, valid)
            loss_GAN = (loss_GAN_AB + loss_GAN_BA) / 2
            
            # å¾ªç’°ä¸€è‡´æ€§æå¤±
            reconstructed_A = generator_BA(fake_B)
            reconstructed_B = generator_AB(fake_A)
            loss_cycle_A = criterion_cycle(reconstructed_A, real_A)
            loss_cycle_B = criterion_cycle(reconstructed_B, real_B)
            loss_cycle = (loss_cycle_A + loss_cycle_B) / 2
            
            # èº«ä»½æå¤±
            identity_A = generator_BA(real_A)
            identity_B = generator_AB(real_B)
            loss_identity_A = criterion_identity(identity_A, real_A)
            loss_identity_B = criterion_identity(identity_B, real_B)
            loss_identity = (loss_identity_A + loss_identity_B) / 2
            
            # ä¿®æ­£ç‰ˆçµæ§‹æå¤±é…å°
            loss_structural = structural_loss(real_A, fake_B, real_B, fake_A)
            
            # éœ§æ¿ƒåº¦åˆ†ç´š (é›¢æ•£åŒ–) - ä¸‰æ®µå¼æ˜ å°„
            haze_density_A = haze_estimator.estimate_haze_density(real_A).clamp(0, 1)  # shape=[B]
            
            # é€£çºŒæ¬Šé‡æ¨¡å¼ - ä½¿ç”¨å†ªå‡½æ•¸å¹³æ»‘èª¿æ•´
            # DCP æ¬Šé‡: 0.15 + 0.3 * (haze_density ** 1.2)
            adaptive_dcp_weight = 0.15 + 0.3 * torch.pow(haze_density_A, 1.2)
            # é™åˆ¶åœ¨ DCPLoss çš„ [w_min, w_max] ç¯„åœå…§ä»¥é¿å…éæ¿€æ¬Šé‡
            adaptive_dcp_weight = adaptive_dcp_weight.clamp(dcp_loss.w_min, dcp_loss.w_max)
            
            # Contrast æ¬Šé‡: åŸºæ–¼éœ§æ¿ƒåº¦çš„é€£çºŒèª¿æ•´
            adaptive_contrast_weight = 0.05 + 0.08 * torch.pow(haze_density_A, 1.1)
            
            # ä¸­éœ§å°ˆå±¬è£œå„Ÿï¼ˆç´‹ç†ä¿ç•™ï¼‰- åœ¨ä¸­ç­‰éœ§æ¿ƒåº¦æ™‚å•Ÿç”¨
            edge_loss = 0.0
            mid_haze_mask = (haze_density_A >= 0.25) & (haze_density_A < 0.7)
            if mid_haze_mask.any():
                # è¨ˆç®—é‚Šç·£ç´‹ç†ä¿ç•™æå¤±
                edge_map_A = torch.abs(real_A[:, :, 1:, :] - real_A[:, :, :-1, :]).mean(dim=1, keepdim=True)
                edge_fake_B = torch.abs(fake_B[:, :, 1:, :] - fake_B[:, :, :-1, :]).mean(dim=1, keepdim=True)
                edge_loss = F.l1_loss(edge_fake_B[:, :, :edge_map_A.size(2), :], edge_map_A)
            
            # æ‡‰ç”¨æš–å•Ÿå‹•ç¸®æ”¾
            adaptive_dcp_weight = adaptive_dcp_weight * warmup_scale
            adaptive_contrast_weight = adaptive_contrast_weight * warmup_scale
            
            # DCP æå¤± - åªæ–½åŠ åœ¨ Aâ†’B (æœ‰éœ§â†’æ¸…æ™°) æ–¹å‘ï¼Œä½¿ç”¨è‡ªé©æ‡‰æ¬Šé‡
            loss_dcp_AB = dcp_loss(fake_B, use_edge_gate=True, override_weight=adaptive_dcp_weight)
            loss_dcp_total = loss_dcp_AB  # ä¸å° fake_A æ–½åŠ  DCP
            
            # å°æ¯”åº¦æå¤± - åªæ–½åŠ åœ¨ Aâ†’B (æœ‰éœ§â†’æ¸…æ™°) æ–¹å‘ï¼Œä½¿ç”¨è‡ªé©æ‡‰æ¬Šé‡
            loss_contrast_AB = contrast_loss(fake_B, override_weight=adaptive_contrast_weight)
            loss_contrast_total = loss_contrast_AB  # ä¸å° fake_A æ–½åŠ å°æ¯”åº¦ç´„æŸ
            
            # TV æå¤±
            loss_tv_AB = tv_loss(fake_B)
            loss_tv_BA = tv_loss(fake_A)
            loss_tv_total = (loss_tv_AB + loss_tv_BA) / 2
            
            # ç¸½ç”Ÿæˆå™¨æå¤±
            loss_G_total = (
                loss_GAN +
                lambda_cycle * loss_cycle +
                lambda_identity * loss_identity +
                lambda_structural * loss_structural +
                loss_dcp_total +
                loss_contrast_total +
                loss_tv_total +
                lambda_edge_preserve * edge_loss  # ä¸­éœ§è£œå„Ÿé … - ä¿ç•™é æ™¯ç´‹ç†
            )
            
            loss_G_total.backward()
            optimizer_G.step()
            
            # -----------------
            # è¨“ç·´åˆ¤åˆ¥å™¨ A
            # -----------------
            
            optimizer_D_A.zero_grad()
            
            # çœŸå¯¦åœ–åƒæå¤±
            pred_real_A = discriminator_A(real_A)
            loss_D_real_A = criterion_GAN(pred_real_A, valid)
            
            # å‡åœ–åƒæå¤±
            pred_fake_A = discriminator_A(fake_A.detach())
            loss_D_fake_A = criterion_GAN(pred_fake_A, fake)
            
            # ç¸½åˆ¤åˆ¥å™¨ A æå¤±
            loss_D_A = (loss_D_real_A + loss_D_fake_A) / 2
            
            loss_D_A.backward()
            optimizer_D_A.step()
            
            # -----------------
            # è¨“ç·´åˆ¤åˆ¥å™¨ B
            # -----------------
            
            optimizer_D_B.zero_grad()
            
            # çœŸå¯¦åœ–åƒæå¤±
            pred_real_B = discriminator_B(real_B)
            loss_D_real_B = criterion_GAN(pred_real_B, valid)
            
            # å‡åœ–åƒæå¤±
            pred_fake_B = discriminator_B(fake_B.detach())
            loss_D_fake_B = criterion_GAN(pred_fake_B, fake)
            
            # ç¸½åˆ¤åˆ¥å™¨ B æå¤±
            loss_D_B = (loss_D_real_B + loss_D_fake_B) / 2
            
            loss_D_B.backward()
            optimizer_D_B.step()
            
            # ç´¯ç©æå¤±
            total_loss_G += loss_G_total.item()
            total_loss_D += (loss_D_A.item() + loss_D_B.item())
            
            # æ‰“å°é€²åº¦ï¼ˆå«é›¢æ•£åˆ†ç´šä¿¡æ¯ï¼‰
            if i % 10 == 0:
                current_haze = haze_density_A.mean().item() if len(haze_density_A.shape) > 0 else haze_density_A.item()
                current_dcp_w = adaptive_dcp_weight.mean().item() if len(adaptive_dcp_weight.shape) > 0 else adaptive_dcp_weight.item()
                
                # çµ±è¨ˆé€£çºŒæ¬Šé‡ç¯„åœ
                dcp_min = adaptive_dcp_weight.min().item() if len(adaptive_dcp_weight.shape) > 0 else adaptive_dcp_weight.item()
                dcp_max = adaptive_dcp_weight.max().item() if len(adaptive_dcp_weight.shape) > 0 else adaptive_dcp_weight.item()
                
                print(f"Epoch [{epoch+1}/{num_epochs}] Batch [{i+1}/{len(dataloader)}] "
                      f"Loss_G: {loss_G_total.item():.4f} "
                      f"Loss_D: {(loss_D_A.item() + loss_D_B.item()):.4f} "
                      f"Cycle: {loss_cycle.item():.4f} "
                      f"DCP: {loss_dcp_total.item():.4f}(w={current_dcp_w:.3f}Â±{dcp_max-dcp_min:.2f}) "
                      f"Edge: {edge_loss:.4f} "
                      f"Haze: {current_haze:.3f} (é€£çºŒæ¨¡å¼)")
        
        # æ›´æ–°å­¸ç¿’ç‡
        scheduler_G.step()
        scheduler_D_A.step()
        scheduler_D_B.step()
        
        # æ¯20å€‹epochä¿å­˜æ¨¡å‹å’Œæ¨£æœ¬
        if (epoch + 1) % 20 == 0:
            checkpoint_name = f"cyclegan_v7_enhanced_epoch_{epoch+1}.pth"
            if not use_self_attention:
                checkpoint_name = f"cyclegan_v7_enhanced_no_attn_epoch_{epoch+1}.pth"
            
            torch.save({
                'epoch': epoch + 1,
                'generator_AB': generator_AB.state_dict(),
                'generator_BA': generator_BA.state_dict(),
                'discriminator_A': discriminator_A.state_dict(),
                'discriminator_B': discriminator_B.state_dict(),
                'optimizer_G': optimizer_G.state_dict(),
                'optimizer_D_A': optimizer_D_A.state_dict(),
                'optimizer_D_B': optimizer_D_B.state_dict(),
                'use_self_attention': use_self_attention
            }, f'checkpoints/{checkpoint_name}')
            
            print(f"âœ… å·²ä¿å­˜æ¨¡å‹: {checkpoint_name}")
            
            # ç”Ÿæˆæ¨£æœ¬åœ–åƒ
            sample_images(generator_AB, generator_BA, dataloader, device, 
                         epoch + 1, 'training_images')
        
        # æ¯å€‹epochçš„çµ±è¨ˆ
        avg_loss_G = total_loss_G / len(dataloader)
        avg_loss_D = total_loss_D / len(dataloader)
        print(f"Epoch [{epoch+1}/{num_epochs}] å®Œæˆ - "
              f"å¹³å‡ Loss_G: {avg_loss_G:.4f}, å¹³å‡ Loss_D: {avg_loss_D:.4f}")
    
    print("ğŸ‰ è¨“ç·´å®Œæˆï¼")

if __name__ == "__main__":
    train_cyclegan()
