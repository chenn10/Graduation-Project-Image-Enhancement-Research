#!/usr/bin/env python3
"""
CycleGAN v6.0 å¤šæ¨¡å‹åœ–åƒå»éœ§ Web æ‡‰ç”¨
æ”¯æŒä¸‹æ‹‰é¸å–®é¸æ“‡ä¸åŒçš„ v6 æ¨¡å‹
"""

import os
import io
import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
from PIL import Image
import torchvision.transforms as transforms
from flask import Flask, request, jsonify, send_file, render_template
from werkzeug.utils import secure_filename
import base64
import cv2
from datetime import datetime
import uuid
import math

app = Flask(__name__)
app.config['MAX_CONTENT_LENGTH'] = 16 * 1024 * 1024  # 16MB max file size
app.config['UPLOAD_FOLDER'] = 'uploads'
app.config['RESULT_FOLDER'] = 'web_results'

# å‰µå»ºå¿…è¦è³‡æ–™å¤¾
os.makedirs(app.config['UPLOAD_FOLDER'], exist_ok=True)
os.makedirs(app.config['RESULT_FOLDER'], exist_ok=True)

# è¨­å‚™è¨­å®š
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

# å¯ç”¨çš„ v6 æ¨¡å‹é…ç½®
V6_MODELS = {
    'v6_epoch_40': {
        'name': 'CycleGAN v6.0 - å¹³è¡¡ç‰ˆæœ¬ (Epoch 40)',
        'path': 'checkpoints/cyclegan_v6_epoch_40.pth',
        'description': 'é©åˆæ—¥å¸¸ä½¿ç”¨çš„å‡è¡¡å»éœ§æ•ˆæœï¼Œè™•ç†é€Ÿåº¦å¿«ï¼Œæ•ˆæœç©©å®šã€‚',
        'badge': 'balanced'
    },
    'v6_epoch_120': {
        'name': 'CycleGAN v6.0 - é«˜ç´šç‰ˆæœ¬ (Epoch 120)',
        'path': 'checkpoints/cyclegan_v6_epoch_120.pth',
        'description': 'æ¨è–¦ä½¿ç”¨ï¼æ›´å¼·çš„å»éœ§èƒ½åŠ›ï¼Œç´°ç¯€ä¿æŒæ›´å¥½ï¼Œé©åˆå¤§éƒ¨åˆ†å ´æ™¯ã€‚',
        'badge': 'premium'
    },
    'v6_epoch_200': {
        'name': 'CycleGAN v6.0 - å®Œæ•´ç‰ˆæœ¬ (Epoch 200)',
        'path': 'checkpoints/cyclegan_v6_epoch_200.pth',
        'description': 'æœ€å¼·å»éœ§æ•ˆæœï¼Œæœ€ä½³ç´°ç¯€é‚„åŸï¼Œé©åˆå°ˆæ¥­ç”¨é€”å’Œé«˜å“è³ªéœ€æ±‚ã€‚',
        'badge': 'ultimate'
    }
}

def spectral_norm(module, name='weight', power_iterations=1):
    """ä½¿ç”¨ PyTorch å…§å»ºçš„ spectral normalization"""
    try:
        return torch.nn.utils.spectral_norm(module, name=name, n_power_iterations=power_iterations)
    except:
        return module

class SelfAttention(nn.Module):
    """ä¿®æ­£çš„è‡ªæ³¨æ„åŠ›æ©Ÿåˆ¶"""
    def __init__(self, in_channels):
        super(SelfAttention, self).__init__()
        self.in_channels = in_channels
        
        # ä½¿ç”¨ spectral norm
        self.query_conv = spectral_norm(nn.Conv2d(in_channels, in_channels // 8, 1))
        self.key_conv = spectral_norm(nn.Conv2d(in_channels, in_channels // 8, 1))
        self.value_conv = spectral_norm(nn.Conv2d(in_channels, in_channels, 1))
        
        self.gamma = nn.Parameter(torch.zeros(1))
        self.softmax = nn.Softmax(dim=-1)
        
    def forward(self, x):
        batch_size, C, H, W = x.size()
        
        # Query, Key, Value
        proj_query = self.query_conv(x).view(batch_size, -1, H * W).permute(0, 2, 1)  # B x N x C'
        proj_key = self.key_conv(x).view(batch_size, -1, H * W)  # B x C' x N
        proj_value = self.value_conv(x).view(batch_size, -1, H * W)  # B x C x N
        
        # æ³¨æ„åŠ›æ¬Šé‡
        attention = torch.bmm(proj_query, proj_key)  # B x N x N
        attention = self.softmax(attention)
        
        # æ‡‰ç”¨æ³¨æ„åŠ›
        out = torch.bmm(proj_value, attention.permute(0, 2, 1))  # B x C x N
        out = out.view(batch_size, C, H, W)  # ä¿®æ­£ï¼šæ­£ç¢ºçš„ view æ“ä½œ
        
        # æ®˜å·®é€£æ¥
        out = self.gamma * out + x
        return out

class ImprovedUpsample(nn.Module):
    """æ”¹é€²çš„ä¸Šæ¡æ¨£å¡Šï¼šUpsample + Conv2d + IN + ReLU"""
    def __init__(self, in_channels, out_channels):
        super(ImprovedUpsample, self).__init__()
        self.upsample = nn.Upsample(scale_factor=2, mode='nearest')
        self.conv = nn.Conv2d(in_channels, out_channels, 3, padding=1)
        self.norm = nn.InstanceNorm2d(out_channels)
        self.relu = nn.ReLU(inplace=True)
        
    def forward(self, x):
        x = self.upsample(x)
        x = self.conv(x)
        x = self.norm(x)
        x = self.relu(x)
        return x

class ResidualBlock(nn.Module):
    """æ®˜å·®å¡Š - ä½¿ç”¨åå°„padding"""
    def __init__(self, channels):
        super(ResidualBlock, self).__init__()
        self.pad1 = nn.ReflectionPad2d(1)
        self.conv1 = nn.Conv2d(channels, channels, 3, padding=0)
        self.norm1 = nn.InstanceNorm2d(channels)
        self.pad2 = nn.ReflectionPad2d(1)
        self.conv2 = nn.Conv2d(channels, channels, 3, padding=0)
        self.norm2 = nn.InstanceNorm2d(channels)
        self.relu = nn.ReLU(inplace=True)
        
    def forward(self, x):
        residual = x
        out = self.pad1(x)
        out = self.conv1(out)
        out = self.norm1(out)
        out = self.relu(out)
        
        out = self.pad2(out)
        out = self.conv2(out)
        out = self.norm2(out)
        return out + residual

class V6Generator(nn.Module):
    """v6 ç”Ÿæˆå™¨ - ä¿®æ­£ä¸Šæ¡æ¨£å’Œæ³¨æ„åŠ›æ©Ÿåˆ¶"""
    def __init__(self, input_channels=3, output_channels=3, n_residual_blocks=9):
        super(V6Generator, self).__init__()
        
        # ç·¨ç¢¼å™¨ - ä½¿ç”¨åå°„padding
        self.encoder = nn.Sequential(
            nn.ReflectionPad2d(3),
            nn.Conv2d(input_channels, 64, 7, padding=0),
            nn.InstanceNorm2d(64),
            nn.ReLU(inplace=True),
            
            nn.Conv2d(64, 128, 3, stride=2, padding=1),
            nn.InstanceNorm2d(128),
            nn.ReLU(inplace=True),
            
            nn.Conv2d(128, 256, 3, stride=2, padding=1),
            nn.InstanceNorm2d(256),
            nn.ReLU(inplace=True),
        )
        
        # æ®˜å·®å¡Š
        residual_layers = []
        for _ in range(n_residual_blocks):
            residual_layers.append(ResidualBlock(256))
        self.residual_blocks = nn.Sequential(*residual_layers)
        
        # è‡ªæ³¨æ„åŠ›ï¼ˆåœ¨ä¸­é–“ç‰¹å¾µåœ–ä¸Šï¼‰
        self.attention = SelfAttention(256)
        
        # è§£ç¢¼å™¨ - å…©æ®µä¸Šæ¡æ¨£ï¼ˆä¿®æ­£å°ºå¯¸å•é¡Œï¼‰+ åå°„padding
        self.decoder = nn.Sequential(
            ImprovedUpsample(256, 128),  # ç¬¬ä¸€æ®µä¸Šæ¡æ¨£ 64x64 -> 128x128
            ImprovedUpsample(128, 64),   # ç¬¬äºŒæ®µä¸Šæ¡æ¨£ 128x128 -> 256x256
            
            nn.ReflectionPad2d(3),
            nn.Conv2d(64, output_channels, 7, padding=0),
            nn.Tanh()
        )
        
    def forward(self, x):
        x = self.encoder(x)
        x = self.residual_blocks(x)
        x = self.attention(x)
        x = self.decoder(x)
        return x

# å…¨åŸŸè®Šæ•¸å­˜å„²æ¨¡å‹
loaded_models = {}

def load_model(model_key):
    """å‹•æ…‹è¼‰å…¥æŒ‡å®šçš„æ¨¡å‹"""
    if model_key in loaded_models:
        return loaded_models[model_key]
    
    if model_key not in V6_MODELS:
        raise ValueError(f"æœªçŸ¥çš„æ¨¡å‹: {model_key}")
    
    model_info = V6_MODELS[model_key]
    model_path = model_info['path']
    
    if not os.path.exists(model_path):
        raise FileNotFoundError(f"æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {model_path}")
    
    print(f"ğŸ”„ è¼‰å…¥æ¨¡å‹: {model_info['name']}")
    
    # å‰µå»ºæ¨¡å‹
    model = V6Generator()
    
    # è¼‰å…¥æ¬Šé‡
    try:
        checkpoint = torch.load(model_path, map_location=device, weights_only=False)
        
        if 'G_A2B_state_dict' in checkpoint:
            model.load_state_dict(checkpoint['G_A2B_state_dict'])
        elif 'model_state_dict' in checkpoint:
            model.load_state_dict(checkpoint['model_state_dict'])
        else:
            model.load_state_dict(checkpoint)
        
        model.to(device)
        model.eval()
        
        # ç·©å­˜æ¨¡å‹
        loaded_models[model_key] = model
        
        print(f"âœ… æˆåŠŸè¼‰å…¥: {model_info['name']}")
        return model
        
    except Exception as e:
        print(f"âŒ è¼‰å…¥å¤±æ•—: {str(e)}")
        raise

class OverlapInference:
    """é‡ç–Šæ¨ç†è™•ç†å¤§åœ–åƒ"""
    def __init__(self, tile_size=256, overlap=32):
        self.tile_size = tile_size
        self.overlap = overlap
        self.stride = tile_size - overlap
        
    def process_large_image(self, model, image_tensor):
        """è™•ç†å¤§åœ–åƒ"""
        B, C, H, W = image_tensor.shape
        
        if H <= self.tile_size and W <= self.tile_size:
            return model(image_tensor)
        
        # è¨ˆç®—éœ€è¦çš„ tiles
        h_tiles = math.ceil(H / self.stride)
        w_tiles = math.ceil(W / self.stride)
        
        # è¼¸å‡ºåœ–åƒ
        output = torch.zeros_like(image_tensor)
        weight_map = torch.zeros((H, W), device=image_tensor.device)
        
        for i in range(h_tiles):
            for j in range(w_tiles):
                # è¨ˆç®—tileä½ç½®
                h_start = i * self.stride
                w_start = j * self.stride
                h_end = min(h_start + self.tile_size, H)
                w_end = min(w_start + self.tile_size, W)
                
                # èª¿æ•´tileå¤§å°ç¢ºä¿ä¸è¶…å‡ºé‚Šç•Œ
                actual_h = h_end - h_start
                actual_w = w_end - w_start
                
                # æå–tile
                tile = image_tensor[:, :, h_start:h_end, w_start:w_end]
                
                # å¦‚æœtileå¤ªå°ï¼Œéœ€è¦padding
                if actual_h < self.tile_size or actual_w < self.tile_size:
                    padded_tile = torch.zeros((B, C, self.tile_size, self.tile_size), 
                                            device=image_tensor.device)
                    padded_tile[:, :, :actual_h, :actual_w] = tile
                    tile_output = model(padded_tile)
                    tile_output = tile_output[:, :, :actual_h, :actual_w]
                else:
                    tile_output = model(tile)
                
                # å‰µå»ºæ¬Šé‡maskï¼ˆä¸­å¿ƒæ¬Šé‡æ›´é«˜ï¼‰
                tile_weight = torch.ones((actual_h, actual_w), device=image_tensor.device)
                
                # é‚Šç·£é™æ¬Š
                if i > 0:  # ä¸æ˜¯ç¬¬ä¸€è¡Œ
                    fade_h = min(self.overlap, actual_h)
                    for k in range(fade_h):
                        weight = k / fade_h
                        tile_weight[k, :] *= weight
                        
                if j > 0:  # ä¸æ˜¯ç¬¬ä¸€åˆ—
                    fade_w = min(self.overlap, actual_w)
                    for k in range(fade_w):
                        weight = k / fade_w
                        tile_weight[:, k] *= weight
                
                # ç´¯åŠ åˆ°è¼¸å‡º
                output[:, :, h_start:h_end, w_start:w_end] += tile_output * tile_weight[None, None, :, :]
                weight_map[h_start:h_end, w_start:w_end] += tile_weight
        
        # æ­¸ä¸€åŒ–
        weight_map = torch.clamp(weight_map, min=1e-8)
        output = output / weight_map[None, None, :, :]
        
        return output

def preprocess_image(image_pil):
    """é è™•ç†åœ–åƒ"""
    # è½‰æ›ç‚ºRGB
    if image_pil.mode != 'RGB':
        image_pil = image_pil.convert('RGB')
    
    # è½‰æ›ç‚ºtensor
    transform = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])
    ])
    
    image_tensor = transform(image_pil).unsqueeze(0).to(device)
    return image_tensor

def postprocess_image(image_tensor):
    """å¾Œè™•ç†åœ–åƒ"""
    # åæ­¸ä¸€åŒ–
    image_tensor = (image_tensor + 1) / 2.0
    image_tensor = torch.clamp(image_tensor, 0, 1)
    
    # è½‰æ›ç‚ºPIL
    image_np = image_tensor.squeeze(0).cpu().detach().numpy()
    image_np = np.transpose(image_np, (1, 2, 0))
    image_np = (image_np * 255).astype(np.uint8)
    
    return Image.fromarray(image_np)

def process_image_with_model(image_pil, model_key):
    """ä½¿ç”¨æŒ‡å®šæ¨¡å‹è™•ç†åœ–åƒ"""
    model = load_model(model_key)
    
    # é è™•ç†
    input_tensor = preprocess_image(image_pil)
    
    # æ¨ç†
    with torch.no_grad():
        if input_tensor.shape[2] > 512 or input_tensor.shape[3] > 512:
            # å¤§åœ–åƒä½¿ç”¨é‡ç–Šæ¨ç†
            overlap_inference = OverlapInference(tile_size=256, overlap=32)
            output_tensor = overlap_inference.process_large_image(model, input_tensor)
        else:
            # å°åœ–åƒç›´æ¥è™•ç†
            output_tensor = model(input_tensor)
    
    # å¾Œè™•ç†
    result_image = postprocess_image(output_tensor)
    return result_image

@app.route('/')
def index():
    """ä¸»é """
    return render_template('v6_optimized_index.html', models=V6_MODELS)

@app.route('/process', methods=['POST'])
def process():
    """è™•ç†åœ–åƒè«‹æ±‚"""
    try:
        # æª¢æŸ¥æ–‡ä»¶
        if 'image' not in request.files:
            return jsonify({'success': False, 'error': 'æœªé¸æ“‡åœ–åƒæ–‡ä»¶'})
        
        file = request.files['image']
        if file.filename == '':
            return jsonify({'success': False, 'error': 'æ–‡ä»¶åç‚ºç©º'})
        
        # æª¢æŸ¥æ¨¡å‹é¸æ“‡
        model_key = request.form.get('model', 'v6_epoch_120')
        if model_key not in V6_MODELS:
            return jsonify({'success': False, 'error': 'ç„¡æ•ˆçš„æ¨¡å‹é¸æ“‡'})
        
        # è®€å–åœ–åƒ
        image_pil = Image.open(io.BytesIO(file.read()))
        original_size = image_pil.size
        
        # è¨˜éŒ„è™•ç†æ™‚é–“
        start_time = datetime.now()
        
        # è™•ç†åœ–åƒ
        result_image = process_image_with_model(image_pil, model_key)
        
        # èª¿æ•´è¼¸å‡ºå°ºå¯¸ç‚ºåŸå§‹å°ºå¯¸
        result_image = result_image.resize(original_size, Image.Resampling.LANCZOS)
        
        end_time = datetime.now()
        processing_time = f"{(end_time - start_time).total_seconds():.2f}ç§’"
        
        # è½‰æ›ç‚ºbase64
        buffered = io.BytesIO()
        result_image.save(buffered, format="JPEG", quality=95)
        img_str = base64.b64encode(buffered.getvalue()).decode()
        result_data_url = f"data:image/jpeg;base64,{img_str}"
        
        return jsonify({
            'success': True,
            'result_image': result_data_url,
            'model_name': V6_MODELS[model_key]['name'],
            'processing_time': processing_time,
            'image_size': f"{original_size[0]}Ã—{original_size[1]}"
        })
        
    except Exception as e:
        print(f"è™•ç†éŒ¯èª¤: {str(e)}")
        return jsonify({'success': False, 'error': f'è™•ç†å¤±æ•—: {str(e)}'})

if __name__ == '__main__':
    print(f"ğŸš€ ä½¿ç”¨è¨­å‚™: {device}")
    print("ğŸŒŸ å•Ÿå‹• CycleGAN V6.0 å¤šæ¨¡å‹å»éœ§æœå‹™")
    print("ğŸ“± è¨ªå•: http://localhost:5000")
    
    app.run(debug=True, host='0.0.0.0', port=5000)